[{"name": "app.py", "content": "from shiny import App, ui, render, module, reactive\n\nimport msprime\nimport numpy\nimport pandas\nimport scipy\nimport demesdraw\n\nimport pynei\nimport msprime_sim_utils\nfrom shiny_modules_general_msprime import (\n    Table,\n    msprime_params_ui,\n    msprime_params_server,\n    input_params_tabs,\n    run_simulation_ui,\n    input_params_server,\n    run_simulation_server,\n)\nfrom shiny_module_sim_demography import (\n    demography_input_accordions,\n    demography_server,\n    open_accordion_panels,\n)\n\nTITLE = \"Admixture\"\nMSPRIME_MODULE_ID = \"msprime\"\nDEMOGRAPHY_MODULE_ID = \"demography\"\nINPUT_PARAMS_MODULE_ID = \"input_params\"\nRUM_SIM_ID = \"run_sim\"\n\ninputs_accordions = []\ninputs_accordions.extend(demography_input_accordions(DEMOGRAPHY_MODULE_ID))\ninputs_accordions.extend(msprime_params_ui(MSPRIME_MODULE_ID))\ninput_accordion = ui.accordion(\n    *inputs_accordions,\n    id=\"inputs_panel\",\n    open=open_accordion_panels,\n)\n\n\npage_elements = [ui.h1(TITLE), input_accordion]\npage_elements.append(input_params_tabs(INPUT_PARAMS_MODULE_ID))\nrun_sim_elements = run_simulation_ui(RUM_SIM_ID)\npage_elements.extend(run_sim_elements)\napp_ui = ui.page_fixed(\n    *page_elements,\n    lang=\"en\",\n)\n\n\ndef server(input, output, session):\n    get_msprime_params = msprime_params_server(MSPRIME_MODULE_ID)\n    get_demography, get_sample_sets = demography_server(\n        DEMOGRAPHY_MODULE_ID, get_msprime_params=get_msprime_params\n    )\n    input_params_server(\n        INPUT_PARAMS_MODULE_ID,\n        get_demography=get_demography,\n        get_msprime_params=get_msprime_params,\n    )\n    run_simulation_server(\n        RUM_SIM_ID,\n        get_sample_sets=get_sample_sets,\n        get_demography=get_demography,\n        get_msprime_params=get_msprime_params,\n    )\n\n\napp = App(app_ui, server)\n", "type": "text"}, {"name": "msprime_sim_utils.py", "content": "import numpy\nimport pandas\nimport msprime\n\nimport pynei\n\n\nclass SimulationResult:\n    def __init__(\n        self, tree_seqs, sample_sets: list[msprime.SampleSet], demography, ploidy\n    ):\n        self.tree_seqs = tree_seqs\n        self._sample_sets = sample_sets\n        self.demography = demography\n        self.ploidy = ploidy\n\n    def _get_pop_ids_and_names(self):\n        tree_seqs = self.tree_seqs\n\n        pop_ids_by_pop_name_in_tseq = {}\n        pop_names_by_pop_id_in_tseq = {}\n        for pop in tree_seqs.populations():\n            pop_name = pop.metadata[\"name\"]\n            pop_id = pop.id\n            pop_ids_by_pop_name_in_tseq[pop_name] = pop_id\n            pop_names_by_pop_id_in_tseq[pop_id] = pop_name\n        return pop_ids_by_pop_name_in_tseq, pop_names_by_pop_id_in_tseq\n\n    def _create_indi_names(self, pop_samples):\n        for pop_sample_name, sample_info in pop_samples.items():\n            sample_info[\"indi_names\"] = [\n                f\"{pop_sample_name}-indi_{node_id}\"\n                for node_id in sample_info[\"sample_node_ids\"]\n            ]\n\n    def _get_pop_samples_info(self):\n        samples = {}\n        tree_seqs = self.tree_seqs\n        pop_ids_by_pop_name_in_tseq, _ = self._get_pop_ids_and_names()\n        for sample in self._sample_sets:\n            pop_name = sample.population\n            sampling_time = sample.time\n            pop_id = pop_ids_by_pop_name_in_tseq[pop_name]\n            sample_name = f\"{pop_name}_{sampling_time}\"\n            sample = {\n                \"sample_node_ids\": tree_seqs.samples(\n                    population=pop_id, time=sampling_time\n                ),\n                \"sample_time\": sampling_time,\n                \"pop_name\": pop_name,\n            }\n            if sample[\"sample_node_ids\"].size == 0:\n                continue\n            samples[sample_name] = sample\n        return samples\n\n    def get_vars_and_pop_samples(self):\n        pop_samples_info = self._get_pop_samples_info()\n\n        pop_sample_by_node_id = {}\n        for pop_sample_name, sample_info in pop_samples_info.items():\n            for node_id in sample_info[\"sample_node_ids\"]:\n                pop_sample_by_node_id[node_id] = pop_sample_name\n\n        tree_seqs = self.tree_seqs\n        haplotype_array = tree_seqs.genotype_matrix()\n        node_ids = tree_seqs.samples()\n        new_shape = (haplotype_array.shape[0], -1, self.ploidy)\n        gt_array = haplotype_array.reshape(new_shape)\n        one_node_id_per_indi = node_ids[:: self.ploidy]\n\n        poss = tree_seqs.tables.sites.position\n\n        vars_info = pandas.DataFrame(\n            {\n                pynei.VAR_TABLE_POS_COL: poss,\n                pynei.VAR_TABLE_CHROM_COL: numpy.full((poss.size,), 1),\n            }\n        )\n\n        indi_names = []\n        indis_by_pop_sample = {\n            pop_sample: [] for pop_sample in set(pop_sample_by_node_id.values())\n        }\n        for node_id in one_node_id_per_indi:\n            pop_sample_name = pop_sample_by_node_id[node_id]\n            indi_name = f\"{node_id}-{pop_sample_name}\"\n            indi_names.append(indi_name)\n            indis_by_pop_sample[pop_sample_name].append(indi_name)\n        vars = pynei.Variants.from_gt_array(\n            gt_array, samples=indi_names, vars_info=vars_info\n        )\n        return {\n            \"vars\": vars,\n            \"indis_by_pop_sample\": indis_by_pop_sample,\n            \"pop_samples_info\": pop_samples_info,\n        }\n\n    @staticmethod\n    def _sort_series_by_sampling_time(series, pop_samples_info):\n        return series.sort_index(\n            key=lambda pop_samples: [\n                -pop_samples_info[pop_sample][\"sample_time\"]\n                for pop_sample in pop_samples\n            ],\n        )\n\n    def _create_series_per_pop_and_dframe(\n        self, series_indexed_by_pop_sample, param, pop_samples_info\n    ):\n        pops = []\n        for pop_sample_name in series_indexed_by_pop_sample.index:\n            pops.append(pop_samples_info[pop_sample_name][\"pop_name\"])\n        dframe = pandas.DataFrame(\n            {\n                param: series_indexed_by_pop_sample,\n                \"pop\": pops,\n            }\n        )\n        series_for_pops = {}\n        for pop, group in dframe.groupby(\"pop\"):\n            values_for_pop = group[param]\n            values_for_pop.index = [\n                pop_samples_info[pop_sample_name][\"sample_time\"]\n                for pop_sample_name in values_for_pop.index\n            ]\n            values_for_pop = values_for_pop.sort_index()\n            series_for_pops[pop] = values_for_pop\n        dframes = []\n        for pop, series in series_for_pops.items():\n            dframe = pandas.DataFrame(\n                {\n                    param: series.values,\n                    \"pop\": [pop] * series.size,\n                    \"generation\": list(series.index),\n                }\n            )\n            dframes.append(dframe)\n        dframe = pandas.concat(dframes)\n\n        return {f\"{param}_by_pop\": series_for_pops, f\"{param}_dframe\": dframe}\n\n    def calc_unbiased_exp_het(self):\n        res = self.get_vars_and_pop_samples()\n        pop_samples_info = res[\"pop_samples_info\"]\n        res = pynei.calc_exp_het_stats_per_var(\n            res[\"vars\"], pops=res[\"indis_by_pop_sample\"], unbiased=True\n        )\n        exp_het = res[\"mean\"]\n        series_indexed_by_pop_sample = exp_het\n        param = \"exp_het\"\n        return self._create_series_per_pop_and_dframe(\n            series_indexed_by_pop_sample, param, pop_samples_info\n        )\n\n    def calc_num_variants(self):\n        res = self.get_vars_and_pop_samples()\n        pop_samples_info = res[\"pop_samples_info\"]\n        res = pynei.calc_poly_vars_ratio_per_var(\n            res[\"vars\"], pops=res[\"indis_by_pop_sample\"]\n        )\n\n        sorted_res = {}\n        for param in [\"num_poly\", \"num_variable\", \"poly_ratio_over_variables\"]:\n            series_indexed_by_pop_sample = res[param]\n            sorted_res[param] = self._create_series_per_pop_and_dframe(\n                series_indexed_by_pop_sample, param, pop_samples_info\n            )\n\n        return sorted_res\n\n    def calc_allele_freq_spectrum(self):\n        res = self.get_vars_and_pop_samples()\n        res = pynei.calc_major_allele_stats_per_var(\n            res[\"vars\"], pops=res[\"indis_by_pop_sample\"], hist_kwargs={\"num_bins\": 20}\n        )\n        return {\"counts\": res[\"hist_counts\"], \"bin_edges\": res[\"hist_bin_edges\"]}\n\n    def calc_exp_het_along_genome(self):\n        n_bins = 15\n        res = self.get_vars_and_pop_samples()\n        vars = res[\"vars\"]\n        chunk = next(vars.iter_vars_chunks())\n        samples = vars.samples\n        pops = res[\"indis_by_pop_sample\"]\n        pops = {\n            pop: numpy.isin(samples, pop_samples) for pop, pop_samples in pops.items()\n        }\n        exp_het_per_var = pynei.diversity._calc_unbiased_exp_het_per_var(\n            chunk, pops=pops\n        )[\"exp_het\"]\n        poss = chunk.vars_info[pynei.VAR_TABLE_POS_COL]\n        bin_edges = numpy.linspace(0, poss.max(), n_bins + 1)\n\n        exp_hets = {}\n        for pos_start, pos_end in zip(bin_edges[:-1], bin_edges[1:]):\n            pos_mask = numpy.logical_and(poss > pos_start, poss <= pos_end)\n            exp_hets_in_genome_segment = exp_het_per_var.loc[pos_mask, :]\n            exp_het_in_genome_segment = exp_hets_in_genome_segment.mean(axis=0)\n            pos = int((pos_start + pos_end) / 2.0)\n            exp_hets[pos] = exp_het_in_genome_segment\n        exp_hets = pandas.DataFrame(exp_hets)\n        return exp_hets\n\n\ndef get_info_from_demography(demography: msprime.Demography):\n    pop_info = {}\n    for population in demography.populations:\n        pop_info[population.name] = {\n            \"id\": population.id,\n            \"name\": population.name,\n            \"initial_size\": population.initial_size,\n        }\n    return {\"pops\": pop_info}\n\n\ndef create_msprime_sample_set(num_samples: int, ploidy: int, pop_name: str, time: int):\n    sample_set = msprime.SampleSet(\n        num_samples=num_samples, population=pop_name, ploidy=ploidy, time=time\n    )\n    return sample_set\n\n\ndef simulate(\n    sample_sets: list[msprime.SampleSet],\n    demography: msprime.Demography,\n    model: None,\n    seq_length_in_bp: int,\n    recomb_rate=1e-8,\n    add_mutations=True,\n    random_seed=None,\n    mutation_rate=1e-8,\n    ploidy=2,\n):\n    kwargs = {\n        \"samples\": sample_sets,\n        \"demography\": demography,\n        \"recombination_rate\": recomb_rate,\n        \"sequence_length\": seq_length_in_bp,\n        \"random_seed\": random_seed,\n    }\n    if model is not None:\n        kwargs[\"model\"] = model\n    tree_seqs = msprime.sim_ancestry(**kwargs)\n    if add_mutations:\n        tree_seqs = msprime.sim_mutations(\n            tree_seqs, rate=mutation_rate, random_seed=random_seed\n        )\n\n    return SimulationResult(\n        tree_seqs=tree_seqs,\n        sample_sets=sample_sets,\n        demography=demography,\n        ploidy=ploidy,\n    )\n", "type": "text"}, {"name": "requirements.txt", "content": "demesdraw\nmore-itertools", "type": "text"}, {"name": "shiny_module_sim_demography.py", "content": "from shiny import ui, module, reactive\n\nimport msprime\nimport msprime_sim_utils\n\nMIN_POP_SIZE = 10\nMAX_POP_SIZE = 10000\nDEF_POP_SIZE = 500\n\nMAX_ANCESTRAL_SPLIT_GENERATION = 3000\nMIN_ANCESTRAL_SPLIT_GENERATION = 200\nDEF_ANCESTRAL_SPLIT_GENERATION = 2500\n\nMAX_ADMIX_GENERATION = MIN_ANCESTRAL_SPLIT_GENERATION - 50\nMIN_ADMIX_GENERATION = 10\nDEF_ADMIX_GENERATION = 10\n\nMIN_POP_A_PROPORTION = 0\nMAX_POP_A_PROPORTION = 1\nDEF_POP_A_PROPORTION = 0.5\n\nNUM_POPS = 3\nPOP_NAMES = [\"pop_a\", \"pop_b\", \"admix\"]\n\nORIG_POP_SIZE_ACCORDION_ID = \"Orig population size\"\nNUM_SPLIT_GENERATION_AGO_ID = \"Times\"\nPOP_SIZE_ACCORDION_ID = \"Split population sizes\"\nNUM_POPS_ACCORDION_ID = \"Num. populations\"\n\n\n@module.ui\ndef demography_input_accordions():\n    pop_a_proportion_slider = ui.input_slider(\n        \"pop_a_proportion_slider\",\n        label=\"Proportion of pop. A in the admixture\",\n        min=MIN_POP_A_PROPORTION,\n        max=MAX_POP_A_PROPORTION,\n        value=DEF_POP_A_PROPORTION,\n        width=\"100%\",\n    )\n\n    admix_generation_slider = ui.input_slider(\n        \"admix_generation_slider\",\n        label=\"Admix. pop. creation time\",\n        min=MIN_ADMIX_GENERATION,\n        max=MAX_ADMIX_GENERATION,\n        value=DEF_ADMIX_GENERATION,\n        width=\"100%\",\n    )\n\n    ancestral_split_generation_slider = ui.input_slider(\n        \"ancestral_split_generation_slider\",\n        label=\"Pop. A and Pop. B split time\",\n        min=MIN_ANCESTRAL_SPLIT_GENERATION,\n        max=MAX_ANCESTRAL_SPLIT_GENERATION,\n        value=DEF_ANCESTRAL_SPLIT_GENERATION,\n        width=\"100%\",\n    )\n\n    pop_size_slider = ui.input_slider(\n        \"pop_size_slider\",\n        label=\"population sizes\",\n        min=MIN_POP_SIZE,\n        max=MAX_POP_SIZE,\n        value=DEF_POP_SIZE,\n        width=\"100%\",\n    )\n\n    accordion_panels = [\n        ui.accordion_panel(\n            \"Simulation parameters\",\n            pop_a_proportion_slider,\n            admix_generation_slider,\n            ancestral_split_generation_slider,\n            pop_size_slider,\n        ),\n    ]\n    return accordion_panels\n\n\nopen_accordion_panels = [POP_SIZE_ACCORDION_ID, NUM_SPLIT_GENERATION_AGO_ID]\n\n\n@module.server\ndef demography_server(input, output, session, get_msprime_params):\n    @reactive.calc\n    def get_demography():\n        ancestral_split_generation = input.ancestral_split_generation_slider()\n        admix_generation = input.admix_generation_slider()\n        pop_sizes = input.pop_size_slider()\n        pop_a_proportion = input.pop_a_proportion_slider()\n        proportions = [pop_a_proportion, 1 - pop_a_proportion]\n        assert ancestral_split_generation > admix_generation\n\n        demography = msprime.Demography()\n        demography.add_population(name=\"pop_a\", initial_size=pop_sizes)\n        demography.add_population(name=\"pop_b\", initial_size=pop_sizes)\n        demography.add_population(name=\"admix\", initial_size=pop_sizes)\n        demography.add_population(name=\"ancestral\", initial_size=pop_sizes)\n        demography.add_admixture(\n            time=admix_generation,\n            derived=\"admix\",\n            ancestral=[\"pop_a\", \"pop_b\"],\n            proportions=proportions,\n        )\n        demography.add_population_split(\n            time=ancestral_split_generation,\n            derived=[\"pop_a\", \"pop_b\"],\n            ancestral=\"ancestral\",\n        )\n        params = {\n            \"Split from ancestral population (generations ago)\": ancestral_split_generation,\n            \"Admixture (generations ago)\": admix_generation,\n            \"Pop. sizes\": pop_sizes,\n            \"Pop. a to pop. b proportion\": pop_a_proportion,\n        }\n        return {\"demography\": demography, \"params_for_table\": params}\n\n    @reactive.calc\n    def get_sample_sets():\n        admix_generation = input.admix_generation_slider()\n        sampling_times = [\n            admix_generation - 1,\n            0,\n        ]\n\n        num_indis_to_sample = get_msprime_params()[\"sample_size\"]\n        sample_sets = []\n        for time in sampling_times:\n            for pop in [\"pop_a\", \"pop_b\", \"admix\"]:\n                sample_set = msprime_sim_utils.create_msprime_sample_set(\n                    num_samples=num_indis_to_sample,\n                    ploidy=2,\n                    pop_name=pop,\n                    time=time,\n                )\n                sample_sets.append(sample_set)\n        return sample_sets\n\n    return get_demography, get_sample_sets\n", "type": "text"}, {"name": "shiny_modules_general_msprime.py", "content": "from array import array\n\nfrom shiny import ui, module, reactive, render\n\nimport numpy\nimport pandas\nimport demesdraw\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\nimport shiny_module_sim_demography\nimport msprime_sim_utils\nfrom style import COLOR_CYCLE, MARKER_CYCLE, LINESTYLES_CYCLE, COLORS\nimport pynei\n\n\nclass Table:\n    def __init__(self, col_names: list[str]):\n        self.col_names = col_names\n        self._cols = {col: [] for col in col_names}\n\n    def add_row(self, row: dict):\n        for col_name, col in self._cols.items():\n            col.append(row[col_name])\n\n    @classmethod\n    def from_series(cls, series: pandas.Series, index_name: str, col_name: str):\n        table = cls([index_name, col_name])\n        for index, value in series.items():\n            table.add_row({index_name: index, col_name: value})\n        return table\n\n    @classmethod\n    def from_dict(cls, dict: list[str, pandas.Series]):\n        table = cls([])\n        table._cols = dict.copy()\n        return table\n\n    @property\n    def df(self):\n        return pandas.DataFrame(self._cols)\n\n\nMIN_SEQ_LENGTH = 5e5\nMAX_SEQ_LENGTH = 10e6\nDEF_SEQ_LENGTH = 2e6\n\nMIN_RECOMB_RATE = -9\nDEF_RECOMB_RATE = -7\nMAX_RECOMB_RATE = -6\n\nMIN_MUT_RATE = -8\nDEF_MUT_RATE = -7\nMAX_MUT_RATE = -6\n\nMIN_SAMPLE_SIZE = 20\nDEF_SAMPLE_SIZE = 50\nMAX_SAMPLE_SIZE = 100\n\nTHIN_LINE = 1\nBROAD_LINE = 2\nUNUSED_STYLE = {\n    \"color\": \"grey\",\n    \"marker\": \".\",\n    \"alpha\": 0.1,\n    \"linewidth\": THIN_LINE,\n    \"marker_filled\": False,\n    \"linestyle\": \"solid\",\n}\n\n######################################################################\n# msprime parameters\n######################################################################\n\n\n@module.ui\ndef msprime_params_ui():\n    sample_size_panel = (\n        (\n            ui.input_slider(\n                \"sample_size_slider\",\n                label=\"\",\n                min=MIN_SAMPLE_SIZE,\n                max=MAX_SAMPLE_SIZE,\n                value=DEF_SAMPLE_SIZE,\n                width=\"100%\",\n            ),\n        ),\n    )\n    recomb_panel = (\n        ui.layout_columns(\n            ui.panel_conditional(\n                \"!input.no_recomb_checkbox\",\n                ui.input_slider(\n                    \"recomb_rate_slider\",\n                    label=\"Log. scale\",\n                    min=MIN_RECOMB_RATE,\n                    max=MAX_RECOMB_RATE,\n                    value=DEF_RECOMB_RATE,\n                    width=\"100%\",\n                ),\n            ),\n            ui.input_checkbox(\n                \"no_recomb_checkbox\", label=\"No recombination\", value=False\n            ),\n            col_widths=(10, 2),\n        ),\n    )\n\n    mut_panel = (\n        (\n            ui.input_slider(\n                \"mut_rate_slider\",\n                label=\"Log. scale\",\n                min=MIN_MUT_RATE,\n                max=MAX_MUT_RATE,\n                value=DEF_MUT_RATE,\n                width=\"100%\",\n            ),\n        ),\n    )\n\n    seq_len_panel = (\n        (\n            ui.input_slider(\n                \"seq_len_slider\",\n                label=\"\",\n                min=MIN_SEQ_LENGTH,\n                max=MAX_SEQ_LENGTH,\n                value=DEF_SEQ_LENGTH,\n                width=\"100%\",\n            ),\n        ),\n    )\n    msprime_accordions = [\n        ui.accordion_panel(\n            \"Sample size (num. individuals to sample)\", sample_size_panel\n        ),\n        ui.accordion_panel(\n            \"Recombination rate (per base pair and generation)\", recomb_panel\n        ),\n        ui.accordion_panel(\"Mutation rate (per base pair and generation)\", mut_panel),\n        ui.accordion_panel(\"Sequence length (in pb)\", seq_len_panel),\n    ]\n    return msprime_accordions\n\n\n@module.server\ndef msprime_params_server(input, output, session):\n    @reactive.calc\n    def get_msprime_params():\n        sample_size = input.sample_size_slider()\n        seq_length_in_bp = input.seq_len_slider()\n        mut_rate = 10 ** input.mut_rate_slider()\n        recomb_rate = 10 ** input.recomb_rate_slider()\n        return {\n            \"sample_size\": sample_size,\n            \"seq_length_in_bp\": seq_length_in_bp,\n            \"mut_rate\": mut_rate,\n            \"recomb_rate\": recomb_rate,\n        }\n\n    return get_msprime_params\n\n\n######################################################################\n# input module\n######################################################################\n\n\n@module.ui\ndef input_params_tabs():\n    demographic_plot = ui.output_plot(\"demographic_plot\")\n    input_params_table = ui.output_data_frame(\"input_params_table\")\n    card_tab = ui.navset_card_tab(\n        ui.nav_panel(\"Demography\", demographic_plot),\n        ui.nav_panel(\"Input parameters summary\", input_params_table),\n    )\n    return card_tab\n\n\n@module.server\ndef input_params_server(input, output, session, get_demography, get_msprime_params):\n    @render.plot(alt=\"Demographic plot\")\n    def demographic_plot():\n        demography = get_demography()[\"demography\"]\n        fig, axes = plt.subplots()\n        axes.plot([1, 1], [0, 0])\n        demesdraw.tubes(demography.to_demes(), ax=axes)\n        return fig\n\n    @render.data_frame\n    def input_params_table():\n        table = Table([\"Parameter\", \"Value\"])\n        demographic_params = get_demography()[\"params_for_table\"]\n        params = demographic_params.copy()\n        params.update(get_msprime_params())\n\n        nice_param_names = {\n            \"sample_size\": \"Sample size (num. individuals sampled at each time)\",\n            \"seq_length_in_bp\": \"Sequence length (in bp)\",\n            \"mut_rate\": \"Mutation rate (per base pair and generation)\",\n            \"recomb_rate\": \"Recombination rate (per base pair and generation)\",\n        }\n\n        for param, value in params.items():\n            param = nice_param_names.get(param, param)\n            table.add_row({\"Parameter\": param, \"Value\": value})\n\n        return render.DataGrid(table.df, width=\"100%\")\n\n\n######################################################################\n# simulation module\n######################################################################\n\nEXP_HET_PLOT_ID = \"exp_het_plot\"\nEXP_HET_TABLE_ID = \"exp_het_table\"\nPOLY_MARKERS_PLOT_ID = \"poly_markers_plot\"\nPOLY_MARKERS_TABLE_ID = \"poly_markers_table\"\n\nLD_PLOT_ID = \"ld_vs_dist\"\nPCA_PLOT_ID = \"pca\"\nAFS_PLOT_ID = \"afs\"\nDIVERSITY_ALONG_GENOME_PLOT_ID = \"diversity_along_genome\"\nPLOT_STRS = (\"Allele freq. spectrum\", \"PCA\", \"LD\", \"Diversity along the genome\")\nPLOT_IDS = (AFS_PLOT_ID, PCA_PLOT_ID, LD_PLOT_ID, DIVERSITY_ALONG_GENOME_PLOT_ID)\n\n\n@module.ui\ndef run_simulation_ui():\n    run_button = ui.input_action_button(\"run_button\", \"Run simulation\")\n\n    exp_het_result = ui.navset_tab(\n        ui.nav_panel(\n            \"Plot\",\n            ui.output_plot(EXP_HET_PLOT_ID),\n            value=\"exp_het_plot\",\n        ),\n        ui.nav_panel(\n            \"Table\",\n            ui.output_data_frame(EXP_HET_TABLE_ID),\n            value=\"exp_het_table\",\n        ),\n        selected=\"exp_het_plot\",\n    )\n\n    poly_ratio_result = ui.navset_tab(\n        ui.nav_panel(\n            \"Plot\",\n            ui.output_plot(\"poly_ratio_over_variables_plot\"),\n            value=\"poly_ratio_over_variables_plot\",\n        ),\n        ui.nav_panel(\n            \"Table\",\n            ui.output_data_frame(\"poly_ratio_over_variables_table\"),\n            value=\"poly_ratio_over_variables_table\",\n        ),\n        selected=\"poly_ratio_over_variables_plot\",\n    )\n    num_poly_result = ui.navset_tab(\n        ui.nav_panel(\n            \"Plot\",\n            ui.output_plot(\"num_poly_plot\"),\n            value=\"num_poly_plot\",\n        ),\n        ui.nav_panel(\n            \"Table\",\n            ui.output_data_frame(\"num_poly_table\"),\n            value=\"num_poly_table\",\n        ),\n        selected=\"num_poly_plot\",\n    )\n    num_variable_result = ui.navset_tab(\n        ui.nav_panel(\n            \"Plot\",\n            ui.output_plot(\"num_variable_plot\"),\n            value=\"num_variable_plot\",\n        ),\n        ui.nav_panel(\n            \"Table\",\n            ui.output_data_frame(\"num_variable_table\"),\n            value=\"num_variable_table\",\n        ),\n        selected=\"num_variable_plot\",\n    )\n\n    nav_panels = [\n        ui.nav_panel(\n            \"Exp. Het.\",\n            exp_het_result,\n        ),\n        ui.nav_panel(\n            \"Polymorphic (95%) ratio over variable\",\n            poly_ratio_result,\n        ),\n        ui.nav_panel(\n            \"Num. polymorphic (95%) variants\",\n            num_poly_result,\n        ),\n        ui.nav_panel(\n            \"Num. variable variants\",\n            num_variable_result,\n        ),\n    ]\n\n    for plot_str, plot_id in zip(\n        PLOT_STRS,\n        PLOT_IDS,\n    ):\n        plot = ui.output_plot(f\"{plot_id}_plot\")\n\n        accordions = []\n        if len(shiny_module_sim_demography.POP_NAMES) > 1:\n            pop_switches = [\n                ui.input_switch(f\"{plot_id}_plot_swicth_pop_{pop}\", pop, value=True)\n                for pop in shiny_module_sim_demography.POP_NAMES\n            ]\n            pops_accordion = ui.accordion(\n                ui.accordion_panel(\"Pops\", pop_switches),\n                id=f\"{plot_id}_pop_switches_accordion\",\n            )\n            accordions.append(pops_accordion)\n        times_accordion = ui.accordion(\n            ui.accordion_panel(\"Times\"),\n            id=f\"{plot_id}_time_switches_accordion\",\n        )\n        accordions.append(times_accordion)\n        sidebar = ui.sidebar(\n            accordions,\n            id=f\"{plot_id}_sidebar\",\n        )\n        sidebar_layout = ui.layout_sidebar(\n            sidebar, plot, position=\"right\", bg=\"#f8f8f8\"\n        )\n        card = ui.card(sidebar_layout)\n        nav_panels.append(ui.nav_panel(plot_str, card))\n\n    output_card = ui.navset_card_tab(*nav_panels)\n\n    return (run_button, output_card)\n\n\n@module.server\ndef run_simulation_server(\n    input, output, session, get_sample_sets, get_demography, get_msprime_params\n):\n    @reactive.calc()\n    @reactive.event(input.run_button)\n    def do_simulation():\n        res = get_demography()\n        demography = res[\"demography\"]\n        model = res.get(\"model\", None)\n\n        sample_sets = get_sample_sets()\n\n        msprime_params = get_msprime_params()\n\n        sim_res = msprime_sim_utils.simulate(\n            sample_sets,\n            demography=demography,\n            model=model,\n            seq_length_in_bp=msprime_params[\"seq_length_in_bp\"],\n            mutation_rate=msprime_params[\"mut_rate\"],\n            recomb_rate=msprime_params[\"recomb_rate\"],\n        )\n        return sim_res\n\n    @reactive.calc\n    def get_sampling_times():\n        sim_res = do_simulation()\n        res = sim_res.get_vars_and_pop_samples()\n        pop_samples_info = res[\"pop_samples_info\"]\n        sampling_times = {\n            sample_info[\"sample_time\"] for sample_info in pop_samples_info.values()\n        }\n        sampling_times = sorted(sampling_times)\n        return sampling_times\n\n    @reactive.effect\n    @reactive.event(input.run_button)\n    def update_time_swithes():\n        sampling_times = get_sampling_times()\n        for plot in PLOT_IDS:\n            ui.remove_accordion_panel(\n                id=f\"{plot}_time_switches_accordion\", target=\"Times\"\n            )\n            switches = [\n                ui.input_switch(\n                    f\"{plot}_plot_swicth_time_{time}\", str(time), value=True\n                )\n                for time in sampling_times\n            ]\n            ui.insert_accordion_panel(\n                id=f\"{plot}_time_switches_accordion\",\n                # target=\"Pops\",\n                panel=ui.accordion_panel(\"Times\", switches),\n            )\n\n    @reactive.calc\n    def get_exp_hets():\n        sim_res = do_simulation()\n        return sim_res.calc_unbiased_exp_het()\n\n    @reactive.calc\n    def get_num_variants():\n        sim_res = do_simulation()\n        num_markers = sim_res.calc_num_variants()\n        return num_markers\n\n    @render.plot(alt=\"Expected heterozygosities\")\n    def exp_het_plot():\n        fig, axes = plt.subplots()\n        res = get_exp_hets()\n\n        axes.set_title(\"Exp. het. over time\")\n        axes.set_xlabel(\"generation\")\n        axes.set_ylabel(\"Exp. het.\")\n        for pop, exp_het in res[\"exp_het_by_pop\"].items():\n            style = get_style_for_pop_and_time(pop=pop)\n            axes.plot(\n                list(-exp_het.index),\n                exp_het.values,\n                label=pop,\n                color=style[\"color\"],\n                linewidth=style[\"linewidth\"],\n                linestyle=style[\"linestyle\"],\n            )\n        axes.set_ylim(0)\n        axes.legend()\n        return fig\n\n    @render.data_frame\n    def exp_het_table():\n        res = get_exp_hets()\n        return render.DataGrid(res[\"exp_het_dframe\"])\n\n    @render.plot(alt=\"Polymorphic (95%) ratio over variable\")\n    def poly_ratio_over_variables_plot():\n        res = get_num_variants()\n        param = \"poly_ratio_over_variables\"\n        res = res[param]\n\n        fig, axes = plt.subplots()\n        axes.set_title(\"Polymorphic (95%) ratio over variable over time\")\n        axes.set_xlabel(\"generation\")\n        axes.set_ylabel(\"Polymorphic (95%) ratio over variable\")\n        for pop, series in res[f\"{param}_by_pop\"].items():\n            style = get_style_for_pop_and_time(pop=pop)\n            axes.plot(\n                list(-series.index),\n                series.values,\n                label=pop,\n                color=style[\"color\"],\n                linewidth=style[\"linewidth\"],\n                linestyle=style[\"linestyle\"],\n            )\n        axes.set_ylim(0)\n        axes.legend()\n        return fig\n\n    @render.data_frame\n    def poly_ratio_over_variables_table():\n        res = get_num_variants()\n        param = \"poly_ratio_over_variables\"\n        res = res[param]\n\n        return render.DataGrid(res[f\"{param}_dframe\"])\n\n    @render.plot(alt=\"Num. polymorphic (95%) variants\")\n    def num_poly_plot():\n        res = get_num_variants()\n        param = \"num_poly\"\n        res = res[param]\n\n        fig, axes = plt.subplots()\n        axes.set_title(\"Num. polymorphic (95%) variants over time\")\n        axes.set_xlabel(\"generation\")\n        axes.set_ylabel(\"Num. polymorphic (95%) variants\")\n        for pop, series in res[f\"{param}_by_pop\"].items():\n            style = get_style_for_pop_and_time(pop=pop)\n            axes.plot(\n                list(-series.index),\n                series.values,\n                label=pop,\n                color=style[\"color\"],\n                linewidth=style[\"linewidth\"],\n                linestyle=style[\"linestyle\"],\n            )\n        axes.set_ylim(0)\n        axes.legend()\n        return fig\n\n    @render.data_frame\n    def num_poly_table():\n        res = get_num_variants()\n        param = \"num_poly\"\n        res = res[param]\n\n        return render.DataGrid(res[f\"{param}_dframe\"])\n\n    @render.plot(alt=\"Num. variants\")\n    def num_variable_plot():\n        res = get_num_variants()\n        param = \"num_variable\"\n        res = res[param]\n\n        fig, axes = plt.subplots()\n        axes.set_title(\"Num. variable variants over time\")\n        axes.set_xlabel(\"generation\")\n        axes.set_ylabel(\"Num. variants\")\n        for pop, series in res[f\"{param}_by_pop\"].items():\n            style = get_style_for_pop_and_time(pop=pop)\n            axes.plot(\n                list(-series.index),\n                series.values,\n                label=pop,\n                color=style[\"color\"],\n                linewidth=style[\"linewidth\"],\n                linestyle=style[\"linestyle\"],\n            )\n        axes.set_ylim(0)\n        axes.legend()\n        return fig\n\n    @render.data_frame\n    def num_variable_table():\n        res = get_num_variants()\n        param = \"num_variable\"\n        res = res[param]\n\n        return render.DataGrid(res[f\"{param}_dframe\"])\n\n    @render.plot(alt=\"Allele frequency spectrum\")\n    def afs_plot():\n        sim_res = do_simulation()\n        fig, axes = plt.subplots()\n        res = sim_res.get_vars_and_pop_samples()\n        pop_samples_info = res[\"pop_samples_info\"]\n        res = sim_res.calc_allele_freq_spectrum()\n        bin_edges = res[\"bin_edges\"]\n        x_poss = (bin_edges[1:] + bin_edges[:-1]) / 2\n        for pop_sample_name, counts in res[\"counts\"].items():\n            pop_sample_info = pop_samples_info[pop_sample_name]\n            generation = pop_sample_info[\"sample_time\"]\n            pop = pop_sample_info[\"pop_name\"]\n            style = get_style(AFS_PLOT_ID, pop=pop, time=generation)\n            axes.plot(\n                x_poss,\n                counts,\n                label=f\"{pop}-{generation}\",\n                color=style[\"color\"],\n                linewidth=style[\"linewidth\"],\n                linestyle=style[\"linestyle\"],\n            )\n        axes.set_xlim((0.5, 1))\n        axes.set_xlabel(\"Allele frequency\")\n        axes.set_ylabel(\"Num. variants\")\n        axes.legend()\n        return fig\n\n    @reactive.calc\n    def get_styles():\n        color_cycle = COLOR_CYCLE\n        marker_cycle = MARKER_CYCLE\n        linestyle_cycle = LINESTYLES_CYCLE\n\n        sim_res = do_simulation()\n        res = sim_res.get_vars_and_pop_samples()\n        pop_samples_info = res[\"pop_samples_info\"]\n\n        pop_names = set()\n        sample_times = set()\n        for sample_info in pop_samples_info.values():\n            pop_names.add(sample_info[\"pop_name\"])\n            sample_times.add(sample_info[\"sample_time\"])\n        pop_names = sorted(pop_names)\n        sample_times = sorted(sample_times)\n\n        if len(sample_times) > 1:\n            alpha_min = 0.3\n            alpha_delta = (1 - alpha_min) / (len(sample_times) - 1)\n        else:\n            alpha_delta = None\n\n        styles = {\"time\": {}, \"pop\": {}}\n        styles[\"default\"] = {\n            \"color\": COLORS[-1],\n            \"alpha\": 1,\n            \"marker\": \"o\",\n            \"marker_filled\": False,\n            \"linewidth\": THIN_LINE,\n            \"linestyle\": \"solid\",\n        }\n\n        for idx, time in enumerate(reversed(sample_times)):\n            style = {\"linewidth\": BROAD_LINE}\n            alpha = 1 if alpha_delta is None else alpha_delta * idx + alpha_min\n            style[\"alpha\"] = alpha\n            style[\"color\"] = next(color_cycle)\n            styles[\"time\"][time] = style\n\n        for pop in pop_names:\n            style = {\"linewidth\": BROAD_LINE}\n            marker_and_fill = next(marker_cycle)\n            style[\"marker\"] = marker_and_fill[0]\n            style[\"marker_filled\"] = marker_and_fill[1]\n            style[\"linestyle\"] = next(linestyle_cycle)\n            styles[\"pop\"][pop] = style\n        return styles\n\n    def get_style_for_pop_and_time(pop=None, time=None):\n        styles = get_styles()\n        if pop is not None:\n            style = styles[\"pop\"][pop]\n        if time is not None:\n            style.update(styles[\"time\"][time])\n        complete_style = {}\n        for trait in (\n            \"color\",\n            \"marker\",\n            \"alpha\",\n            \"linewidth\",\n            \"marker_filled\",\n            \"linestyle\",\n        ):\n            complete_style[trait] = style.get(trait, styles[\"default\"][trait])\n        return complete_style\n\n    @render.plot(alt=\"Principal Component Analysis\")\n    def pca_plot():\n        sim_res = do_simulation()\n        fig, axes = plt.subplots()\n\n        max_maf = 0.95\n\n        res = sim_res.get_vars_and_pop_samples()\n        vars = res[\"vars\"]\n        vars = pynei.filter_by_maf(vars, max_allowed_maf=max_maf)\n        pop_samples_info = res[\"pop_samples_info\"]\n        indis_by_pop_sample = res[\"indis_by_pop_sample\"]\n\n        try:\n            pca_res = pynei.pca.do_pca_with_vars(vars, transform_to_biallelic=True)\n        except Exception as error:\n            if \"DLASCL\" in str(error):\n                raise RuntimeError(\n                    \" PCA calculation failed, please rerun the simulation\\n(This is a bug in OpenBLAS: On entry to DLASCL parameter number 4 had an illegal value)\"\n                )\n            else:\n                raise error\n        projections = pca_res[\"projections\"]\n        explained_variance = pca_res[\"explained_variance (%)\"]\n\n        filter_stats = pynei.gather_filtering_stats(vars)\n\n        pop_sample_names = sorted(\n            pop_samples_info.keys(),\n            key=lambda pop: pop_samples_info[pop][\"sample_time\"],\n        )\n\n        indi_names = projections.index.to_numpy()\n        x_values = projections.iloc[:, 0].values\n        y_values = projections.iloc[:, 1].values\n        for pop_sample_name in pop_sample_names:\n            mask = numpy.isin(indi_names, indis_by_pop_sample[pop_sample_name])\n            pop_sample_info = pop_samples_info[pop_sample_name]\n\n            time = pop_sample_info[\"sample_time\"]\n            pop = pop_sample_info[\"pop_name\"]\n            style = get_style(PCA_PLOT_ID, pop=pop, time=time)\n\n            facecolor = style[\"color\"] if style[\"marker_filled\"] else \"none\"\n\n            axes.scatter(\n                x_values[mask],\n                y_values[mask],\n                label=f\"{pop}-{time}\",\n                color=style[\"color\"],\n                marker=style[\"marker\"],\n                alpha=style[\"alpha\"],\n                facecolor=facecolor,\n            )\n        axes.set_title(\n            f\"PCA done with {filter_stats['maf']['vars_kept']} polymorphic ({int(max_maf*100)}%) variations\"\n        )\n        axes.set_xlabel(f\"PC1 ({explained_variance.iloc[0]:.2f}%)\")\n        axes.set_ylabel(f\"PC2 ({explained_variance.iloc[1]:.2f}%)\")\n        axes.legend()\n        return fig\n\n    def get_style(plot_id, time, pop):\n        time_switch_id = f\"{plot_id}_plot_swicth_time_{time}\"\n        time_input_switch = getattr(input, time_switch_id)\n        pop_input_switch_id = f\"{plot_id}_plot_swicth_pop_{pop}\"\n\n        if len(shiny_module_sim_demography.POP_NAMES) > 1:\n            pop_input_switch = getattr(input, pop_input_switch_id)\n            pop_is_on = pop_input_switch()\n        else:\n            pop_is_on = True\n\n        if pop_is_on and time_input_switch():\n            style = get_style_for_pop_and_time(pop=pop, time=time)\n        else:\n            style = UNUSED_STYLE\n        return style\n\n    @reactive.calc\n    def calc_ld_curves():\n        sim_res = do_simulation()\n        fig, axes = plt.subplots()\n\n        res = sim_res.get_vars_and_pop_samples()\n        vars = res[\"vars\"]\n        indis_by_pop_sample = res[\"indis_by_pop_sample\"]\n        pop_samples_info = res[\"pop_samples_info\"]\n\n        ld_curves = {}\n\n        for pop_sample_name, lds_and_dists in pynei.get_ld_and_dist_for_pops(\n            vars, indis_by_pop_sample, max_allowed_maf=0.90\n        ).items():\n            dists = array(\"f\")\n            r2s = array(\"f\")\n            for r2, dist in lds_and_dists:\n                r2s.append(r2)\n                dists.append(dist)\n            r2s = numpy.array(r2s, dtype=float)\n            dists = numpy.array(dists, dtype=float)\n\n            dist_delta = 0.01 * (dists.max() - dists.min())\n\n            lowess_dists_lds = sm.nonparametric.lowess(r2s, dists, delta=dist_delta)\n            lowess_dists = lowess_dists_lds[:, 0]\n            lowess_lds = lowess_dists_lds[:, 1]\n\n            xs = numpy.linspace(0, dists.max(), 50)\n            interpolated_r2 = numpy.interp(xs, lowess_dists, lowess_lds)\n\n            pop_sample_info = pop_samples_info[pop_sample_name]\n            time = pop_sample_info[\"sample_time\"]\n            pop = pop_sample_info[\"pop_name\"]\n            ld_curves[pop_sample_name] = {\n                \"dists\": xs,\n                \"r2s\": interpolated_r2,\n                \"time\": time,\n                \"pop\": pop,\n            }\n        return ld_curves\n\n    @render.plot(alt=\"LD vs dist plot\")\n    def ld_vs_dist_plot():\n        fig, axes = plt.subplots()\n        for pop_sample_name, lds_and_dists in calc_ld_curves().items():\n            time = lds_and_dists[\"time\"]\n            pop = lds_and_dists[\"pop\"]\n            dists = lds_and_dists[\"dists\"]\n            r2s = lds_and_dists[\"r2s\"]\n            style = get_style(LD_PLOT_ID, time, pop)\n            axes.plot(\n                dists,\n                r2s,\n                label=f\"{pop}-{time}\",\n                color=style[\"color\"],\n                alpha=style[\"alpha\"],\n                linewidth=style[\"linewidth\"],\n                linestyle=style[\"linestyle\"],\n            )\n        axes.set_xlabel(\"Dist. between markers (bp.)\")\n        axes.set_ylabel(\"Rogers & Huff r2\")\n        axes.legend()\n        return fig\n\n    @render.plot(alt=\"Diversity along the genome plot\")\n    def diversity_along_genome_plot():\n        sim_res = do_simulation()\n        res = sim_res.get_vars_and_pop_samples()\n        pop_samples_info = res[\"pop_samples_info\"]\n        exp_hets = sim_res.calc_exp_het_along_genome()\n\n        fig, axes = plt.subplots()\n        for sampling, sampling_exp_hets in exp_hets.iterrows():\n            pop_sample_info = pop_samples_info[sampling]\n            time = pop_sample_info[\"sample_time\"]\n            pop = pop_sample_info[\"pop_name\"]\n            style = get_style(DIVERSITY_ALONG_GENOME_PLOT_ID, time, pop)\n            axes.plot(\n                sampling_exp_hets.index,\n                sampling_exp_hets.values,\n                label=f\"{pop}-{time}\",\n                color=style[\"color\"],\n                alpha=style[\"alpha\"],\n                linewidth=style[\"linewidth\"],\n                linestyle=style[\"linestyle\"],\n            )\n        axes.set_ylim((0, axes.get_ylim()[1]))\n        axes.set_xlabel(\"Genomic position (bp)\")\n        axes.set_ylabel(\"Mean unbiased exp. het.\")\n        return fig\n", "type": "text"}, {"name": "style.py", "content": "import itertools\n\nimport matplotlib.pyplot as plt\n\nCOLORS = list(plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"])\nCOLOR_CYCLE = itertools.cycle(COLORS)\n\nMARKERS = [\n    (\"o\", True),\n    (\"x\", True),\n    (\"s\", False),\n    (\"v\", True),\n    (\"^\", True),\n    (\"<\", True),\n    (\">\", True),\n    (\"p\", True),\n    (\"*\", True),\n    (\"h\", True),\n    (\"H\", True),\n    (\"D\", True),\n    (\"d\", True),\n]\nMARKER_CYCLE = itertools.cycle(MARKERS)\n\nLINESTYLES = [\"solid\", \"dashed\", \"dotted\", \"-.\"]\nLINESTYLES_CYCLE = itertools.cycle(LINESTYLES)\n", "type": "text"}, {"name": "pynei/__init__.py", "content": "from pynei.variants import Variants\nfrom pynei.config import VAR_TABLE_POS_COL, VAR_TABLE_CHROM_COL\nfrom pynei.gt_counts import calc_obs_het_stats_per_var, calc_major_allele_stats_per_var\nfrom pynei.diversity import calc_exp_het_stats_per_var, calc_poly_vars_ratio_per_var\nfrom pynei.pca import do_pca_with_vars, do_pca, do_pcoa\nfrom pynei.dists import calc_pairwise_kosman_dists\nfrom pynei.io_vcf import vars_from_vcf\nfrom pynei.var_filters import (\n    filter_by_missing_data,\n    filter_by_maf,\n    gather_filtering_stats,\n)\nfrom pynei.ld import get_ld_and_dist_for_pops\n", "type": "text"}, {"name": "pynei/config.py", "content": "from enum import Enum\nimport array\n\nimport pandas\nimport numpy\n\nMIN_NUM_GENOTYPES_FOR_POP_STAT = 20\nDEF_POLY_THRESHOLD = 0.95\n\nVAR_TABLE_CHROM_COL = \"chrom\"\nVAR_TABLE_POS_COL = \"pos\"\nVAR_TABLE_QUAL_COL = \"qual\"\nVAR_TABLE_ID_COL = \"id\"\n\nPANDAS_FLOAT_DTYPE = pandas.Float32Dtype\nPANDAS_INT_DTYPE = pandas.Int32Dtype\nPANDAS_POS_DTYPE = pandas.UInt64Dtype\nPANDAS_STR_DTYPE = pandas.StringDtype\n\nPYTHON_ARRAY_TYPE = \"i\"\nBYTE_SIZE_OF_INT = array.array(PYTHON_ARRAY_TYPE, [0]).itemsize\nMAX_ALLELE_NUMBER = {1: 127, 2: 32767, 4: 2147483647}[BYTE_SIZE_OF_INT]\nGT_NUMPY_DTYPE = {2: numpy.int16, 4: numpy.int32}[BYTE_SIZE_OF_INT]\n\nPANDAS_STRING_STORAGE = (\n    \"python\"  # when pyodide supports pyarrow, we will change this to 'pyarrow'\n)\n# PANDAS_STRING_STORAGE = \"pyarrow\"\nDEF_NUM_VARS_PER_CHUNK = 10000\nLINEAL = \"lineal\"\nLOGARITHMIC = \"logarithmic\"\nBinType = Enum(\"BinType\", [LINEAL, LOGARITHMIC])\nDEF_POP_NAME = \"pop\"\nMIN_NUM_SAMPLES_FOR_POP_STAT = 20\nMISSING_ALLELE = -1\n\nDEF_NUMPY_GZIP_COMPRESSION_LEVEL = 4\n", "type": "text"}, {"name": "pynei/dists.py", "content": "import math\nfrom typing import Sequence\nimport itertools\nimport random\nfrom functools import partial\n\nimport numpy\nimport pandas\n\nfrom pynei.pipeline import Pipeline\nfrom pynei.config import MIN_NUM_SAMPLES_FOR_POP_STAT\nfrom pynei.utils_pop import _calc_pops_idxs\nfrom pynei.gt_counts import _count_alleles_per_var, _calc_obs_het_per_var\n\n\ndef _get_vector_from_square(square_dists):\n    num_indis = square_dists.shape[0]\n    len_vector = (num_indis**2 - num_indis) // 2\n    dist_vector = numpy.empty((len_vector,), dtype=square_dists.dtype)\n    current_pos = 0\n    for row_idx in range(num_indis):\n        this_row_items = square_dists[row_idx, row_idx + 1 :]\n        start = current_pos\n        stop = current_pos + this_row_items.shape[0]\n        dist_vector[start:stop] = this_row_items\n        current_pos = stop\n    return dist_vector\n\n\ndef _calc_num_indis_from_dist_vector(dist_vector_size):\n    a = 1\n    b = -1\n    c = -2 * dist_vector_size\n    num_indis = int((-b + math.sqrt(b**2 - 4 * a * c)) / (2 * a))\n    return num_indis\n\n\ndef _get_square_from_vector(dist_vector):\n    num_indis = _calc_num_indis_from_dist_vector(dist_vector.size)\n    square = numpy.zeros((num_indis, num_indis), dtype=dist_vector.dtype)\n\n    col_start = 1\n    vector_start = 0\n    for row_idx in range(num_indis):\n        num_row_items = num_indis - row_idx - 2\n        col_stop = col_start + num_row_items + 1\n        vector_stop = vector_start + num_row_items + 1\n        items = dist_vector[vector_start:vector_stop]\n        square[row_idx, col_start:col_stop] = items\n\n        reversed_items = items\n        r_col_idx = row_idx\n        r_row_start = col_start\n        r_row_stop = col_stop\n        square[r_row_start:r_row_stop, r_col_idx] = reversed_items\n\n        col_start += 1\n        vector_start = vector_stop\n    return square\n\n\nclass Distances:\n    def __init__(\n        self,\n        dist_vector: numpy.ndarray,\n        names: Sequence[str] | Sequence[int] | None = None,\n    ):\n        self.dist_vector = numpy.array(dist_vector)\n        self.dist_vector.flags.writeable = False\n\n        expected_num_indis = _calc_num_indis_from_dist_vector(self.dist_vector.shape[0])\n        if names is None:\n            names = numpy.arange(expected_num_indis)\n        else:\n            names = numpy.array(names)\n            if names.size != expected_num_indis:\n                raise ValueError(\n                    f\"Expected num indis ({expected_num_indis}) does not match the given number of names ({names.shape})\"\n                )\n        names.flags.writeable = False\n        self.names = names\n\n    @classmethod\n    def from_square_dists(cls, dists: pandas.DataFrame):\n        if dists.shape[0] != dists.shape[1]:\n            raise ValueError(\n                f\"A square dist matrix is required, but shape was not squared: {dists.shape}\"\n            )\n        names = numpy.array(dists.index)\n        dist_vector = _get_vector_from_square(dists.values)\n        return cls(dist_vector=dist_vector, names=names)\n\n    @property\n    def square_dists(self):\n        dists = _get_square_from_vector(self.dist_vector)\n        dists = pandas.DataFrame(dists, index=self.names, columns=self.names)\n        return dists\n\n    @property\n    def triang_list_of_lists(self):\n        dist_vector = iter(self.dist_vector)\n        length = 0\n        dists = []\n        while True:\n            dist_row = list(itertools.islice(dist_vector, length))\n            if length and not dist_row:\n                break\n            dist_row.append(0)\n            dists.append(dist_row)\n            length += 1\n        return dists\n\n\nclass _KosmanDistCalculator:\n    def __init__(self, chunk):\n        \"\"\"It calculates the pairwise distance between individuals using the Kosman-Leonard dist\n\n        The Kosman distance is explained in \"Similarity coefficients for molecular markers in\n        studies of genetic relationships between individuals for haploid, diploid, and polyploid\n        species\"\n        Kosman, Leonard (2005) Mol. Ecol. (DOI: 10.1111/j.1365-294X.2005.02416.x)\n        \"\"\"\n\n        self.indi_names = _get_samples_from_variants(chunk)\n        gt_array = chunk.gts.gt_values\n        self.gt_array = gt_array\n        self.allele_is_missing = chunk.gts.missing_mask\n\n    def _get_sample_gts(self, indi_i, indi_j):\n        gt_i = self.gt_array[:, indi_i, :]\n        is_missing_i = numpy.sum(self.allele_is_missing[:, indi_i, :], axis=1) > 0\n\n        gt_j = self.gt_array[:, indi_j, :]\n        is_missing_j = numpy.sum(self.allele_is_missing[:, indi_j, :], axis=1) > 0\n\n        is_called = numpy.logical_not(numpy.logical_or(is_missing_i, is_missing_j))\n\n        gt_i = gt_i[is_called, ...]\n        gt_j = gt_j[is_called, ...]\n        return gt_i, gt_j\n\n    def calc_dist_btw_two_indis(self, indi_i, indi_j):\n        dist_sum, n_snps = self.calc_dist_sum_and_n_snps_btw_two_indis(indi_i, indi_j)\n        return dist_sum / n_snps\n\n    def calc_dist_sum_and_n_snps_btw_two_indis(self, indi_i, indi_j):\n        gt_i, gt_j = self._get_sample_gts(indi_i, indi_j)\n\n        if gt_i.shape[1] != 2:\n            raise ValueError(\"Only diploid are allowed\")\n\n        alleles_comparison1 = gt_i == gt_j.transpose()[:, :, None]\n        alleles_comparison2 = gt_j == gt_i.transpose()[:, :, None]\n\n        result = numpy.add(\n            numpy.any(alleles_comparison2, axis=2).sum(axis=0),\n            numpy.any(alleles_comparison1, axis=2).sum(axis=0),\n        )\n\n        result2 = numpy.full(result.shape, fill_value=0.5)\n        result2[result == 0] = 1\n        result2[result == 4] = 0\n        return result2.sum(), result2.shape[0]\n\n    @property\n    def num_items(self):\n        n_indis = self.gt_array.shape[1]\n        if self.indi_names is not None:\n            assert n_indis == len(self.indi_names)\n\n        return n_indis\n\n\ndef _calc_pairwise_dists_between_pops(\n    dist_between_items_calculator,\n    pop1_samples=None,\n    pop2_samples=None,\n):\n    if (pop1_samples is not None and pop2_samples is None) or (\n        pop1_samples is None and pop2_samples is not None\n    ):\n        raise ValueError(\n            \"When pop1_samples or pop2_samples are given both should be given\"\n        )\n\n    if pop1_samples is None:\n        n_samples = dist_between_items_calculator.num_items\n        num_dists_to_calculate = int((n_samples**2 - n_samples) / 2)\n        dists_sum = numpy.zeros(num_dists_to_calculate)\n        n_snps_matrix = numpy.zeros(num_dists_to_calculate)\n    else:\n        shape = (len(pop1_samples), len(pop2_samples))\n        dists_sum = numpy.zeros(shape)\n        n_snps_matrix = numpy.zeros(shape)\n\n    indi_names = dist_between_items_calculator.indi_names\n    calc_dist_between_two_indis = (\n        dist_between_items_calculator.calc_dist_sum_and_n_snps_btw_two_indis\n    )\n\n    if pop1_samples is None:\n        sample_combinations = itertools.combinations(range(n_samples), 2)\n    else:\n        pop1_indi_idxs = [\n            idx for idx, sample in enumerate(indi_names) if sample in pop1_samples\n        ]\n        pop2_indi_idxs = [\n            idx for idx, sample in enumerate(indi_names) if sample in pop2_samples\n        ]\n        sample_combinations = itertools.product(pop1_indi_idxs, pop2_indi_idxs)\n\n    index = 0\n    for sample_i, sample_j in sample_combinations:\n        dist_sum, n_snps = calc_dist_between_two_indis(sample_i, sample_j)\n\n        if pop1_samples is None:\n            dists_sum[index] = dist_sum\n            n_snps_matrix[index] = n_snps\n            index += 1\n        else:\n            dists_samplei_idx = pop1_indi_idxs.index(sample_i)\n            dists_samplej_idx = pop2_indi_idxs.index(sample_j)\n            dists_sum[dists_samplei_idx, dists_samplej_idx] = dist_sum\n            n_snps_matrix[dists_samplei_idx, dists_samplej_idx] = n_snps\n\n    if pop1_samples is not None:\n        dists_sum = pandas.DataFrame(\n            dists_sum, index=pop1_samples, columns=pop2_samples\n        )\n\n    return dists_sum, n_snps_matrix\n\n\ndef _calc_kosman_dist_for_chunk(chunk, pop1_samples=None, pop2_samples=None):\n    dist_between_items_calculator = _KosmanDistCalculator(chunk)\n    return _calc_pairwise_dists_between_pops(\n        dist_between_items_calculator,\n        pop1_samples=pop1_samples,\n        pop2_samples=pop2_samples,\n    )\n\n\ndef _reduce_kosman_dists(acummulated_dists_and_snps, new_dists_and_snps):\n    new_dists, new_n_snps = new_dists_and_snps\n    if acummulated_dists_and_snps is None:\n        abs_distances = new_dists_and_snps[0].copy()\n        n_snps_matrix = new_dists_and_snps[1]\n    else:\n        abs_distances, n_snps_matrix = acummulated_dists_and_snps\n        abs_distances = numpy.add(abs_distances, new_dists)\n        n_snps_matrix = numpy.add(n_snps_matrix, new_n_snps)\n    return abs_distances, n_snps_matrix\n\n\ndef _calc_pairwise_dists_exact(\n    variants,\n    dist_pipeline,\n    num_processes=2,\n):\n    dists = dist_pipeline.map_and_reduce(variants, num_processes=num_processes)\n    return dists\n\n\ndef _get_dists(\n    variants,\n    dist_pipeline,\n    cached_dists=None,\n    num_processes=2,\n):\n    pop1_samples = dist_pipeline.pop1_samples\n    pop2_samples = dist_pipeline.pop2_samples\n    if cached_dists is None:\n        samples_to_calc_dists_from = pop1_samples\n    else:\n        assert all(numpy.equal(pop2_samples, cached_dists.columns))\n        samples_to_calc_dists_from = pop1_samples[\n            numpy.logical_not(numpy.isin(pop1_samples, cached_dists.index))\n        ]\n\n    if samples_to_calc_dists_from.size:\n        new_dists = _calc_pairwise_dists_exact(\n            variants,\n            dist_pipeline,\n            num_processes=num_processes,\n        )\n    else:\n        new_dists = None\n\n    if cached_dists is None:\n        cached_dists = new_dists\n        dists = new_dists\n    else:\n        if new_dists is not None:\n            cached_dists = pandas.concat([new_dists, cached_dists], axis=\"index\")\n        dists = cached_dists.loc[pandas.Index(pop1_samples), cached_dists.columns]\n    return dists, cached_dists\n\n\ndef _get_samples_from_variants(variants):\n    if variants.samples is None:\n        samples = numpy.arange(variants.num_samples)\n    else:\n        samples = numpy.array(variants.samples)\n    return samples\n\n\ndef _select_seed_samples_for_embedding(\n    variants,\n    num_initial_samples,\n    max_num_seed_expansions,\n    min_num_snps=None,\n    num_processes=2,\n):\n    all_samples = _get_samples_from_variants(variants)\n\n    num_samples = all_samples.size\n    if not num_initial_samples:\n        num_initial_samples = int(round(math.log2(num_samples) ** 2))\n    seed_samples = numpy.array(random.sample(list(all_samples), k=num_initial_samples))\n\n    cached_dists = None\n    for _ in range(max_num_seed_expansions):\n        dist_pipeline = _create_kosman_dist_pipeline(\n            pop1_samples=seed_samples,\n            pop2_samples=all_samples,\n            min_num_snps=min_num_snps,\n        )\n        seed_dists, cached_dists = _get_dists(\n            variants,\n            dist_pipeline,\n            num_processes=num_processes,\n            cached_dists=None,\n        )\n\n        sample_idxs_with_max_dists_to_seeds = numpy.argmax(seed_dists, axis=1)\n        most_distant_samples = numpy.unique(\n            all_samples[sample_idxs_with_max_dists_to_seeds]\n        )\n        # print(most_distant_samples)\n        dist_pipeline = _create_kosman_dist_pipeline(\n            pop1_samples=most_distant_samples,\n            pop2_samples=all_samples,\n            min_num_snps=min_num_snps,\n        )\n        dists_to_most_distant_samples, cached_dists = _get_dists(\n            variants,\n            dist_pipeline,\n            num_processes=num_processes,\n            cached_dists=cached_dists,\n        )\n        samples_idxs_most_distant_to_most_distant_samples = numpy.argmax(\n            dists_to_most_distant_samples.values, axis=1\n        )\n        samples_most_distant_to_most_distant_samples = numpy.unique(\n            all_samples[samples_idxs_most_distant_to_most_distant_samples]\n        )\n        # print(samples_most_distant_to_most_distant_samples)\n        old_num_seeds = seed_samples.size\n        seed_samples = numpy.union1d(\n            seed_samples, samples_most_distant_to_most_distant_samples\n        )\n        new_num_seeds = seed_samples.size\n        if old_num_seeds == new_num_seeds:\n            break\n    return seed_samples, cached_dists\n\n\ndef _calc_pairwise_dists_btw_all_and_some_ref_indis(\n    variants,\n    min_num_snps=None,\n    num_initial_samples=None,\n    max_num_seed_expansions=5,\n    num_processes=2,\n):\n    # following \"Sequence embedding for fast construction of guide trees for multiple sequence alignment\"\n    # Blackshields, Algorithms for Molecular Biology (2010). https://doi.org/10.1186/1748-7188-5-21\n    # https://almob.biomedcentral.com/articles/10.1186/1748-7188-5-21\n\n    seed_samples, cached_dists = _select_seed_samples_for_embedding(\n        variants,\n        num_initial_samples,\n        max_num_seed_expansions,\n        min_num_snps=min_num_snps,\n        num_processes=num_processes,\n    )\n    all_samples = _get_samples_from_variants(variants)\n\n    dist_pipeline = _create_kosman_dist_pipeline(\n        pop1_samples=seed_samples,\n        pop2_samples=all_samples,\n        min_num_snps=min_num_snps,\n    )\n    dists_for_embedding, _ = _get_dists(\n        variants,\n        dist_pipeline,\n        cached_dists=cached_dists,\n        num_processes=num_processes,\n    )\n    dists_btw_all_indis_and_some_ref_indis = pandas.DataFrame(\n        dists_for_embedding.T, index=all_samples\n    )\n\n    return dists_btw_all_indis_and_some_ref_indis\n\n\nclass _EuclideanCalculator:\n    def __init__(self, sample_data):\n        self.sample_data = sample_data\n        self.indi_names = list(sample_data.index)\n\n    def calc_dist_between_two_indis(self, indi_i, indi_j):\n        dists, _ = self.calc_dist_sum_and_n_snps_btw_two_indis(self, indi_i, indi_j)\n        return dists\n\n    def calc_dist_sum_and_n_snps_btw_two_indis(self, indi_i, indi_j):\n        # just to have the same interface as the Kosman distance\n        a = self.sample_data.iloc[indi_i, :]\n        b = self.sample_data.iloc[indi_j, :]\n        dist = numpy.linalg.norm(a - b)\n        n_snps = 0\n        return dist, n_snps\n\n    @property\n    def num_items(self):\n        return len(self.indi_names)\n\n\ndef _calc_euclidean_pairwise_dists(sample_data: pandas.DataFrame):\n    dist_between_items_calculator = _EuclideanCalculator(sample_data=sample_data)\n    dists, _ = _calc_pairwise_dists_between_pops(dist_between_items_calculator)\n    return dists\n\n\ndef calc_euclidean_pairwise_dists(sample_data: pandas.DataFrame):\n    return Distances(_calc_euclidean_pairwise_dists(sample_data), sample_data.index)\n\n\ndef _calc_pairwise_dists_using_embedding(variants, num_processes=2, min_num_snps=None):\n    dists_between_all_indis_and_some_ref_indis = (\n        _calc_pairwise_dists_btw_all_and_some_ref_indis(\n            variants,\n            min_num_snps=min_num_snps,\n            num_processes=num_processes,\n        )\n    )\n    return _calc_euclidean_pairwise_dists(dists_between_all_indis_and_some_ref_indis)\n\n\ndef _calc_pairwise_dists(\n    variants,\n    num_processes=2,\n    min_num_snps=None,\n    use_approx_embedding_algorithm=False,\n):\n    if use_approx_embedding_algorithm:\n        dists = _calc_pairwise_dists_using_embedding(\n            variants,\n            num_processes=num_processes,\n            min_num_snps=min_num_snps,\n        )\n    else:\n        pipeline = _create_kosman_dist_pipeline(min_num_snps=min_num_snps)\n        dists = _calc_pairwise_dists_exact(\n            variants,\n            dist_pipeline=pipeline,\n            num_processes=num_processes,\n        )\n    dists = Distances(dists, _get_samples_from_variants(variants))\n    return dists\n\n\ndef _kosman_calc_after_reduce(reduced_result, min_num_snps=None):\n    abs_distances, n_snps_matrix = reduced_result\n\n    if min_num_snps is not None:\n        n_snps_matrix[n_snps_matrix < min_num_snps] = numpy.nan\n\n    with numpy.errstate(invalid=\"ignore\"):\n        dists = abs_distances / n_snps_matrix\n    return dists\n\n\ndef _create_kosman_dist_pipeline(\n    pop1_samples=None, pop2_samples=None, min_num_snps=None\n):\n    calc_kosman_dist_for_chunk = partial(\n        _calc_kosman_dist_for_chunk,\n        pop1_samples=pop1_samples,\n        pop2_samples=pop2_samples,\n    )\n    map_dist_functs = [calc_kosman_dist_for_chunk]\n    reduce_dist_funct = _reduce_kosman_dists\n    after_reduce_funct = partial(_kosman_calc_after_reduce, min_num_snps=min_num_snps)\n    dist_pipeline = Pipeline(\n        map_functs=map_dist_functs,\n        reduce_funct=reduce_dist_funct,\n        after_reduce_funct=after_reduce_funct,\n    )\n    dist_pipeline.pop1_samples = pop1_samples\n    dist_pipeline.pop2_samples = pop2_samples\n    return dist_pipeline\n\n\ndef calc_pairwise_kosman_dists(\n    variants, min_num_snps=None, use_approx_embedding_algorithm=False\n) -> Distances:\n    \"\"\"It calculates the distance between individuals using the Kosman\n    distance.\n\n    The Kosman distance is explained in DOI: 10.1111/j.1365-294X.2005.02416.x\n    \"\"\"\n\n    return _calc_pairwise_dists(\n        variants,\n        min_num_snps=min_num_snps,\n        num_processes=1,\n        use_approx_embedding_algorithm=use_approx_embedding_algorithm,\n    )\n\n\ndef hmean(array, axis=0, dtype=None):\n    # Harmonic mean only defined if greater than zero\n    if isinstance(array, numpy.ma.MaskedArray):\n        size = array.count(axis)\n    else:\n        if axis is None:\n            array = array.ravel()\n            size = array.shape[0]\n        else:\n            size = array.shape[axis]\n    with numpy.errstate(divide=\"ignore\"):\n        inverse_mean = numpy.sum(1.0 / array, axis=axis, dtype=dtype)\n    is_inf = numpy.logical_not(numpy.isfinite(inverse_mean))\n    hmean = size / inverse_mean\n    hmean[is_inf] = numpy.nan\n\n    return hmean\n\n\ndef _calc_pairwise_dest(\n    chunk, pop_idxs, sorted_pop_ids, alleles, min_num_genotypes, ploidy\n):\n    debug = False\n\n    num_pops = 2\n    pop1, pop2 = sorted_pop_ids\n\n    res = _count_alleles_per_var(\n        chunk,\n        pops=pop_idxs,\n        calc_freqs=True,\n        alleles=None,\n        min_num_samples=min_num_genotypes,\n    )\n    allele_freq1 = res[\"counts\"][pop1][\"allelic_freqs\"].values\n    allele_freq2 = res[\"counts\"][pop2][\"allelic_freqs\"].values\n\n    exp_het1 = 1 - numpy.sum(allele_freq1**ploidy, axis=1)\n    exp_het2 = 1 - numpy.sum(allele_freq2**ploidy, axis=1)\n    hs_per_var = (exp_het1 + exp_het2) / 2\n    if debug:\n        print(\"hs_per_var\", hs_per_var)\n\n    global_allele_freq = (allele_freq1 + allele_freq2) / 2\n    global_exp_het = 1 - numpy.sum(global_allele_freq**ploidy, axis=1)\n    ht_per_var = global_exp_het\n    if debug:\n        print(\"ht_per_var\", ht_per_var)\n\n    res = _calc_obs_het_per_var(chunk, pops=pop_idxs)\n    obs_het_per_var = res[\"obs_het_per_var\"]\n    obs_het1 = obs_het_per_var[pop1].values\n    obs_het2 = obs_het_per_var[pop2].values\n    if debug:\n        print(f\"{obs_het1=}\")\n        print(f\"{obs_het2=}\")\n    called_gts_per_var = res[\"called_gts_per_var\"]\n    called_gts1 = called_gts_per_var[pop1]\n    called_gts2 = called_gts_per_var[pop2]\n\n    called_gts = numpy.array([called_gts1, called_gts2])\n    try:\n        called_gts_hmean = hmean(called_gts, axis=0)\n    except ValueError:\n        called_gts_hmean = None\n\n    if called_gts_hmean is None:\n        num_vars = chunk.num_vars\n        corrected_hs = numpy.full((num_vars,), numpy.nan)\n        corrected_ht = numpy.full((num_vars,), numpy.nan)\n    else:\n        mean_obs_het_per_var = numpy.nanmean(numpy.array([obs_het1, obs_het2]), axis=0)\n        corrected_hs = (called_gts_hmean / (called_gts_hmean - 1)) * (\n            hs_per_var - (mean_obs_het_per_var / (2 * called_gts_hmean))\n        )\n        if debug:\n            print(\"mean_obs_het_per_var\", mean_obs_het_per_var)\n            print(\"corrected_hs\", corrected_hs)\n        corrected_ht = (\n            ht_per_var\n            + (corrected_hs / (called_gts_hmean * num_pops))\n            - (mean_obs_het_per_var / (2 * called_gts_hmean * num_pops))\n        )\n        if debug:\n            print(\"corrected_ht\", corrected_ht)\n\n        not_enough_gts = numpy.logical_or(\n            called_gts1 < min_num_genotypes, called_gts2 < min_num_genotypes\n        )\n        corrected_hs[not_enough_gts] = numpy.nan\n        corrected_ht[not_enough_gts] = numpy.nan\n\n    num_vars_in_chunk = numpy.count_nonzero(~numpy.isnan(corrected_hs))\n    hs_in_chunk = numpy.nansum(corrected_hs)\n    ht_in_chunk = numpy.nansum(corrected_ht)\n    return {\n        \"hs\": hs_in_chunk,\n        \"ht\": ht_in_chunk,\n        \"num_vars\": num_vars_in_chunk,\n        \"hs_per_var\": corrected_hs,\n        \"ht_per_var\": corrected_ht,\n    }\n\n\nclass _DestPopHsHtCalculator:\n    def __init__(self, pop_idxs, sorted_pop_ids, alleles, min_num_genotypes, ploidy):\n        self.pop_idxs = pop_idxs\n        self.pop_ids = sorted_pop_ids\n        self.alleles = alleles\n        self.min_num_genotypes = min_num_genotypes\n        self.ploidy = ploidy\n\n    def __call__(self, chunk):\n        pop_idxs = self.pop_idxs\n        pop_ids = self.pop_ids\n        num_pops = len(pop_ids)\n\n        corrected_hs = pandas.DataFrame(\n            numpy.zeros(shape=(num_pops, num_pops), dtype=float),\n            columns=pop_ids,\n            index=pop_ids,\n        )\n        corrected_ht = pandas.DataFrame(\n            numpy.zeros(shape=(num_pops, num_pops), dtype=float),\n            columns=pop_ids,\n            index=pop_ids,\n        )\n        num_vars = pandas.DataFrame(\n            numpy.zeros(shape=(num_pops, num_pops), dtype=int),\n            columns=pop_ids,\n            index=pop_ids,\n        )\n\n        for pop1, pop2 in itertools.combinations(self.pop_ids, 2):\n            res = _calc_pairwise_dest(\n                chunk,\n                sorted_pop_ids=(pop1, pop2),\n                pop_idxs=pop_idxs,\n                alleles=self.alleles,\n                min_num_genotypes=self.min_num_genotypes,\n                ploidy=self.ploidy,\n            )\n            corrected_hs.loc[pop1, pop2] = res[\"hs\"]\n            corrected_ht.loc[pop1, pop2] = res[\"ht\"]\n            num_vars.loc[pop1, pop2] = res[\"num_vars\"]\n            corrected_hs.loc[pop2, pop1] = res[\"hs\"]\n            corrected_ht.loc[pop2, pop1] = res[\"ht\"]\n            num_vars.loc[pop2, pop1] = res[\"num_vars\"]\n        return {\"hs\": corrected_hs, \"ht\": corrected_ht, \"num_vars\": num_vars}\n\n\ndef _calc_jost_from_ht_hs_per_var(sorted_pop_ids, hs, ht):\n    assert len(sorted_pop_ids) == 2\n    num_pops = 2\n    dest = (num_pops / (num_pops - 1)) * ((ht - hs) / (1 - hs))\n    return dest\n\n\nclass _DestDistCalculator(_DestPopHsHtCalculator):\n    def __call__(self, chunk):\n        res = _calc_pairwise_dest(\n            chunk=chunk,\n            sorted_pop_ids=self.pop_ids,\n            pop_idxs=self.pop_idxs,\n            alleles=None,\n            min_num_genotypes=self.min_num_genotypes,\n            ploidy=self.ploidy,\n        )\n        dists_per_var = _calc_jost_from_ht_hs_per_var(\n            self.pop_ids, hs=res[\"hs_per_var\"], ht=res[\"ht_per_var\"]\n        )\n        return dists_per_var\n\n\ndef _accumulate_dest_results(accumulated_result, new_result):\n    if accumulated_result is None:\n        accumulated_hs = new_result[\"hs\"]\n        accumulated_ht = new_result[\"ht\"]\n        total_num_vars = new_result[\"num_vars\"]\n    else:\n        accumulated_hs = accumulated_result[\"hs\"] + new_result[\"hs\"]\n        accumulated_ht = accumulated_result[\"ht\"] + new_result[\"ht\"]\n        total_num_vars = accumulated_result[\"num_vars\"] + new_result[\"num_vars\"]\n    return {\"hs\": accumulated_hs, \"ht\": accumulated_ht, \"num_vars\": total_num_vars}\n\n\ndef _calc_jost_from_ht_hs(sorted_pop_ids, hs, ht, num_vars):\n    tot_n_pops = len(sorted_pop_ids)\n    dists = numpy.empty(int((tot_n_pops**2 - tot_n_pops) / 2))\n    dists[:] = numpy.nan\n    num_pops = 2\n    for idx, (pop_id1, pop_id2) in enumerate(itertools.combinations(sorted_pop_ids, 2)):\n        with numpy.errstate(invalid=\"ignore\"):\n            corrected_hs = hs.loc[pop_id1, pop_id2] / num_vars.loc[pop_id1, pop_id2]\n            corrected_ht = ht.loc[pop_id1, pop_id2] / num_vars.loc[pop_id1, pop_id2]\n        dest = (num_pops / (num_pops - 1)) * (\n            (corrected_ht - corrected_hs) / (1 - corrected_hs)\n        )\n        dists[idx] = dest\n    return dists\n\n\ndef calc_jost_dest_pop_dists(\n    variants,\n    pops: dict[list[str]],\n    alleles: list[int] | None = None,\n    min_num_samples=MIN_NUM_SAMPLES_FOR_POP_STAT,\n) -> Distances:\n    \"\"\"This is an implementation of the formulas proposed in GenAlex\"\"\"\n\n    pop_idxs = _calc_pops_idxs(pops, variants.samples)\n    sorted_pop_ids = sorted(pop_idxs.keys())\n    calc_dest_dists = _DestPopHsHtCalculator(\n        pop_idxs=pop_idxs,\n        sorted_pop_ids=sorted_pop_ids,\n        alleles=alleles,\n        min_num_genotypes=min_num_samples,\n        ploidy=variants.ploidy,\n    )\n\n    pipeline = Pipeline(\n        map_functs=[calc_dest_dists],\n        reduce_funct=_accumulate_dest_results,\n    )\n\n    res = pipeline.map_and_reduce(variants)\n    accumulated_hs = res[\"hs\"]\n    accumulated_ht = res[\"ht\"]\n    num_vars = res[\"num_vars\"]\n\n    dists = _calc_jost_from_ht_hs(\n        sorted_pop_ids, accumulated_hs, accumulated_ht, num_vars\n    )\n\n    dists = Distances(dists, sorted_pop_ids)\n    return dists\n", "type": "text"}, {"name": "pynei/diversity.py", "content": "from functools import partial\nfrom typing import Sequence\n\nimport numpy\nimport pandas\n\nfrom pynei.config import (\n    MIN_NUM_SAMPLES_FOR_POP_STAT,\n    DEF_POLY_THRESHOLD,\n)\nfrom pynei.gt_counts import _count_alleles_per_var, _calc_maf_per_var\nfrom pynei.utils_pop import _calc_pops_idxs\nfrom pynei.utils_stats import _calc_stats_per_var\nfrom pynei.pipeline import Pipeline\n\n\ndef _calc_exp_het_per_var(\n    chunk, pops, min_num_samples=MIN_NUM_SAMPLES_FOR_POP_STAT, ploidy=None\n):\n    if ploidy is None:\n        ploidy = chunk.ploidy\n\n    res = _count_alleles_per_var(\n        chunk,\n        pops=pops,\n        calc_freqs=True,\n        min_num_samples=min_num_samples,\n    )\n\n    sorted_pops = sorted(pops.keys())\n\n    missing_allelic_gts = {\n        pop_id: res[\"counts\"][pop_id][\"missing_gts_per_var\"] for pop_id in sorted_pops\n    }\n    missing_allelic_gts = pandas.DataFrame(missing_allelic_gts, columns=sorted_pops)\n\n    exp_het = {}\n    for pop_id in sorted_pops:\n        allele_freqs = res[\"counts\"][pop_id][\"allelic_freqs\"].values\n        exp_het[pop_id] = 1 - numpy.sum(allele_freqs**ploidy, axis=1)\n    exp_het = pandas.DataFrame(exp_het, columns=sorted_pops)\n\n    return {\"exp_het\": exp_het, \"missing_allelic_gts\": missing_allelic_gts}\n\n\ndef _calc_unbiased_exp_het_per_var(\n    chunk, pops, min_num_samples=MIN_NUM_SAMPLES_FOR_POP_STAT, ploidy=None\n):\n    \"Calculated using Unbiased Heterozygosity (Codom Data) Genalex formula\"\n    if ploidy is None:\n        ploidy = chunk.ploidy\n\n    res = _calc_exp_het_per_var(\n        chunk,\n        pops=pops,\n        min_num_samples=min_num_samples,\n        ploidy=ploidy,\n    )\n    exp_het = res[\"exp_het\"]\n\n    missing_allelic_gts = res[\"missing_allelic_gts\"]\n\n    num_allelic_gtss = []\n    for pop in missing_allelic_gts.columns:\n        pop_slice = pops[pop]\n        if (\n            isinstance(pop_slice, slice)\n            and pop_slice.start is None\n            and pop_slice.stop is None\n            and pop_slice.step is None\n        ):\n            num_allelic_gts = chunk.num_samples * ploidy\n        else:\n            num_allelic_gts = len(pops[pop]) * ploidy\n        num_allelic_gtss.append(num_allelic_gts)\n    num_exp_allelic_gts_per_pop = numpy.array(num_allelic_gtss)\n    num_called_allelic_gts_per_snp = (\n        num_exp_allelic_gts_per_pop[numpy.newaxis, :] - missing_allelic_gts\n    )\n    num_samples = num_called_allelic_gts_per_snp / ploidy\n\n    unbiased_exp_het = (2 * num_samples / (2 * num_samples - 1)) * exp_het\n    return {\n        \"exp_het\": unbiased_exp_het,\n        \"missing_allelic_gts\": missing_allelic_gts,\n    }\n\n\ndef calc_exp_het_stats_per_var(\n    variants,\n    pops: dict[str, Sequence[str] | Sequence[int]] | None = None,\n    min_num_samples=MIN_NUM_SAMPLES_FOR_POP_STAT,\n    ploidy=None,\n    hist_kwargs=None,\n    unbiased=True,\n):\n    if hist_kwargs is None:\n        hist_kwargs = {}\n    hist_kwargs[\"range\"] = hist_kwargs.get(\"range\", (0, 1))\n\n    samples = variants.samples\n    pops = _calc_pops_idxs(pops, samples)\n\n    if unbiased:\n        calc_het_funct = _calc_unbiased_exp_het_per_var\n    else:\n        calc_het_funct = _calc_exp_het_per_var\n\n    return _calc_stats_per_var(\n        variants=variants,\n        calc_stats_for_chunk=partial(\n            calc_het_funct,\n            pops=pops,\n            min_num_samples=min_num_samples,\n            ploidy=ploidy,\n        ),\n        get_stats_for_chunk_result=lambda x: x[\"exp_het\"],\n        hist_kwargs=hist_kwargs,\n    )\n\n\ndef _calc_num_poly_vars(\n    chunk,\n    poly_threshold=DEF_POLY_THRESHOLD,\n    pops: dict[str, Sequence[str] | Sequence[int]] | None = None,\n    min_num_samples=MIN_NUM_SAMPLES_FOR_POP_STAT,\n):\n    res = _calc_maf_per_var(\n        chunk,\n        pops=pops,\n        min_num_samples=min_num_samples,\n    )\n    mafs = res[\"major_allele_freqs_per_var\"]\n\n    num_not_nas = mafs.notna().sum(axis=0)\n    num_variable = (mafs < 1).sum(axis=0)\n    num_poly = (mafs < poly_threshold).sum(axis=0)\n    res = {\n        \"num_poly\": num_poly,\n        \"num_variable\": num_variable,\n        \"tot_num_variants_with_data\": num_not_nas,\n    }\n    return res\n\n\ndef _accumulate_pop_sums(\n    accumulated_result: pandas.DataFrame | None, next_result: pandas.DataFrame\n):\n    if accumulated_result is None:\n        accumulated_result = next_result\n    else:\n        accumulated_result = {\n            param: accumulated_result[param] + values\n            for param, values in next_result.items()\n        }\n    return accumulated_result\n\n\ndef calc_poly_vars_ratio_per_var(\n    variants,\n    poly_threshold=DEF_POLY_THRESHOLD,\n    pops: dict[str, Sequence[str] | Sequence[int]] | None = None,\n    min_num_samples=MIN_NUM_SAMPLES_FOR_POP_STAT,\n):\n    samples = variants.samples\n    pops = _calc_pops_idxs(pops, samples)\n\n    calc_num_poly_vars = partial(\n        _calc_num_poly_vars,\n        poly_threshold=poly_threshold,\n        pops=pops,\n        min_num_samples=min_num_samples,\n    )\n\n    pipeline = Pipeline(\n        map_functs=[calc_num_poly_vars],\n        reduce_funct=_accumulate_pop_sums,\n    )\n    res = pipeline.map_and_reduce(variants)\n\n    num_poly = res[\"num_poly\"]\n    num_variable = res[\"num_variable\"]\n    num_not_nas = res[\"tot_num_variants_with_data\"]\n\n    poly_ratio = num_poly / num_not_nas\n    poly_ratio2 = num_poly / num_variable\n\n    res = {\n        \"num_poly\": num_poly,\n        \"poly_ratio\": poly_ratio,\n        \"poly_ratio_over_variables\": poly_ratio2,\n        \"num_variable\": num_variable,\n        \"tot_num_variants_with_data\": num_not_nas,\n    }\n    return res\n", "type": "text"}, {"name": "pynei/gt_counts.py", "content": "from functools import partial\n\nimport numpy\nimport pandas\n\nfrom pynei.config import MIN_NUM_SAMPLES_FOR_POP_STAT, DEF_POP_NAME, MISSING_ALLELE\nfrom pynei.utils_pop import _calc_pops_idxs\nfrom pynei.utils_stats import _calc_stats_per_var\n\n\ndef _calc_gt_is_missing(chunk, partial_res=None):\n    res = {} if partial_res is None else partial_res\n    if \"gt_is_missing\" in res:\n        return res\n\n    allele_is_missing = chunk.gts.missing_mask\n    res[\"gt_is_missing\"] = numpy.any(allele_is_missing, axis=2)\n    return res\n\n\ndef _calc_gt_is_het(chunk, partial_res=None):\n    res = {} if partial_res is None else partial_res\n    if \"gt_is_het\" in res:\n        return res\n\n    res = _calc_gt_is_missing(chunk, partial_res=res)\n    gt_is_missing = res[\"gt_is_missing\"]\n\n    gt_array = chunk.gts.gt_values\n    gt_is_het = numpy.logical_not(\n        numpy.all(gt_array == gt_array[:, :, 0][:, :, numpy.newaxis], axis=2)\n    )\n    res[\"gt_is_het\"] = numpy.logical_and(gt_is_het, numpy.logical_not(gt_is_missing))\n    return res\n\n\ndef _calc_obs_het_per_var(chunk, pops):\n    res = _calc_gt_is_het(chunk)\n    gt_is_het = res[\"gt_is_het\"]\n    gt_is_missing = res[\"gt_is_missing\"]\n\n    obs_het_per_var = {}\n    called_gts_per_var = {}\n    for pop_name, pop_slice in pops.items():\n        num_vars_het_per_var = numpy.sum(gt_is_het[:, pop_slice], axis=1)\n        gt_is_missing_for_pop = gt_is_missing[:, pop_slice]\n        num_samples = gt_is_missing_for_pop.shape[1]\n        num_non_missing_per_var = num_samples - numpy.sum(\n            gt_is_missing[:, pop_slice], axis=1\n        )\n        with numpy.errstate(invalid=\"ignore\"):\n            obs_het_per_var[pop_name] = num_vars_het_per_var / num_non_missing_per_var\n        called_gts_per_var[pop_name] = num_non_missing_per_var\n\n    obs_het_per_var = pandas.DataFrame(obs_het_per_var)\n    called_gts_per_var = pandas.DataFrame(called_gts_per_var)\n    return {\n        \"obs_het_per_var\": obs_het_per_var,\n        \"called_gts_per_var\": called_gts_per_var,\n    }\n\n\ndef calc_obs_het_stats_per_var(\n    variants,\n    pops: list[str] | None = None,\n    hist_kwargs=None,\n):\n    if hist_kwargs is None:\n        hist_kwargs = {}\n    hist_kwargs[\"range\"] = hist_kwargs.get(\"range\", (0, 1))\n\n    pops = _calc_pops_idxs(pops, variants.samples)\n\n    return _calc_stats_per_var(\n        variants=variants,\n        calc_stats_for_chunk=partial(_calc_obs_het_per_var, pops=pops),\n        get_stats_for_chunk_result=lambda x: x[\"obs_het_per_var\"],\n        hist_kwargs=hist_kwargs,\n    )\n\n\ndef _count_alleles_per_var(\n    chunk,\n    calc_freqs: bool,\n    pops: dict[str, list[int]] | None = None,\n    alleles=None,\n    min_num_samples=MIN_NUM_SAMPLES_FOR_POP_STAT,\n):\n    gts = chunk.gts.gt_values\n    missing_mask = chunk.gts.missing_mask\n\n    alleles_in_chunk = set(numpy.unique(gts).tolist()).difference([MISSING_ALLELE])\n    alleles = sorted(alleles_in_chunk)\n    ploidy = chunk.ploidy\n\n    if pops is None:\n        pops = {DEF_POP_NAME: slice(None, None)}\n\n    if alleles is not None:\n        if alleles_in_chunk.difference(alleles):\n            raise RuntimeError(\n                f\"These gts have alleles ({alleles_in_chunk}) not present in the given ones ({alleles})\"\n            )\n\n    result = {}\n    for pop_id, pop_slice in pops.items():\n        pop_gts = gts[:, pop_slice, :]\n        pop_missing_mask = missing_mask[:, pop_slice, :]\n        allele_counts = numpy.empty(\n            shape=(pop_gts.shape[0], len(alleles)), dtype=numpy.int32\n        )\n        for idx, allele in enumerate(alleles):\n            is_allele = numpy.logical_and(\n                pop_gts == allele, numpy.logical_not(pop_missing_mask)\n            )\n            allele_counts_per_row = numpy.sum(is_allele, axis=(1, 2))\n            allele_counts[:, idx] = allele_counts_per_row\n        allele_counts = pandas.DataFrame(allele_counts, columns=alleles)\n        missing_counts = numpy.sum(pop_missing_mask, axis=(1, 2))\n\n        result[pop_id] = {\n            \"allele_counts\": allele_counts,\n            \"missing_gts_per_var\": missing_counts,\n        }\n\n        if calc_freqs:\n            expected_num_allelic_gts_in_snp = pop_gts.shape[1] * pop_gts.shape[2]\n            num_allelic_gts_per_snp = expected_num_allelic_gts_in_snp - missing_counts\n            num_allelic_gts_per_snp = num_allelic_gts_per_snp.reshape(\n                (num_allelic_gts_per_snp.shape[0], 1)\n            )\n            allelic_freqs_per_snp = allele_counts / num_allelic_gts_per_snp\n            num_gts_per_snp = (\n                num_allelic_gts_per_snp.reshape((num_allelic_gts_per_snp.size,))\n                / ploidy\n            )\n            not_enough_data = num_gts_per_snp < min_num_samples\n            allelic_freqs_per_snp[not_enough_data] = numpy.nan\n\n            result[pop_id][\"allelic_freqs\"] = allelic_freqs_per_snp\n\n    return {\"counts\": result, \"alleles\": alleles_in_chunk}\n\n\ndef _calc_maf_per_var(\n    chunk,\n    pops,\n    min_num_samples=MIN_NUM_SAMPLES_FOR_POP_STAT,\n):\n    res = _count_alleles_per_var(\n        chunk,\n        pops=pops,\n        alleles=None,\n        calc_freqs=True,\n        min_num_samples=min_num_samples,\n    )\n    major_allele_freqs = {}\n    for pop, pop_res in res[\"counts\"].items():\n        pop_allelic_freqs = pop_res[\"allelic_freqs\"]\n        major_allele_freqs[pop] = pop_allelic_freqs.max(axis=1)\n    major_allele_freqs = pandas.DataFrame(major_allele_freqs)\n    return {\"major_allele_freqs_per_var\": major_allele_freqs}\n\n\ndef calc_major_allele_stats_per_var(\n    variants,\n    pops: list[str] | None = None,\n    min_num_samples=MIN_NUM_SAMPLES_FOR_POP_STAT,\n    hist_kwargs=None,\n):\n    if hist_kwargs is None:\n        hist_kwargs = {}\n    hist_kwargs[\"range\"] = hist_kwargs.get(\"range\", (0, 1))\n\n    samples = variants.samples\n    pops = _calc_pops_idxs(pops, samples)\n\n    return _calc_stats_per_var(\n        variants=variants,\n        calc_stats_for_chunk=partial(\n            _calc_maf_per_var, pops=pops, min_num_samples=min_num_samples\n        ),\n        get_stats_for_chunk_result=lambda x: x[\"major_allele_freqs_per_var\"],\n        hist_kwargs=hist_kwargs,\n    )\n", "type": "text"}, {"name": "pynei/io_vars.py", "content": "from pathlib import Path\nimport json\nimport gzip\n\nimport numpy\nimport pandas\n\nfrom pynei.variants import Variants, Genotypes, VariantsChunk\nimport pynei.config as config\n\n\ndef _create_vars_info_path(chunk_dir):\n    return chunk_dir / \"vars_info.parquet\"\n\n\ndef _create_gt_path(chunk_dir):\n    return chunk_dir / \"gts.npy.gz\"\n\n\ndef _create_gt_mask_path(chunk_dir):\n    return chunk_dir / \"gt_mask.npy.gz\"\n\n\ndef _create_metadata_path(output_dir):\n    return output_dir / \"var_dir_metadata.json\"\n\n\ndef write_vars(\n    vars: Variants,\n    output_dir: Path,\n    numpy_array_compression_level=config.DEF_NUMPY_GZIP_COMPRESSION_LEVEL,\n):\n    output_dir = Path(output_dir)\n\n    metadata = {\n        \"var_dir_format_version\": \"1.0\",\n        \"var_chunks_metadata\": [],\n        \"samples\": vars.samples,\n        \"num_samples\": vars.num_samples,\n        \"ploidy\": vars.ploidy,\n    }\n\n    for chunk_idx, chunk in enumerate(vars.iter_vars_chunks()):\n        chunk_dir = output_dir / f\"chunk_{chunk_idx:04d}\"\n        chunk_dir.mkdir()\n        chunk_metadata = {\"dir\": str(chunk_dir.relative_to(output_dir))}\n\n        vars_info = chunk.vars_info\n        if vars_info is not None:\n            fpath = str(_create_vars_info_path(chunk_dir))\n            fhand = open(fpath, \"wb\")\n            chunk.vars_info.to_parquet(fhand)\n            if (\n                config.VAR_TABLE_CHROM_COL in vars_info.columns\n                and config.VAR_TABLE_POS_COL in vars_info.columns\n            ):\n                chunk_metadata[\"start_chrom\"] = vars_info[\n                    config.VAR_TABLE_CHROM_COL\n                ].iloc[0]\n                chunk_metadata[\"start_pos\"] = int(\n                    vars_info[config.VAR_TABLE_POS_COL].iloc[0]\n                )\n                chunk_metadata[\"end_chrom\"] = vars_info[\n                    config.VAR_TABLE_CHROM_COL\n                ].iloc[-1]\n                chunk_metadata[\"end_pos\"] = int(\n                    vars_info[config.VAR_TABLE_POS_COL].iloc[-1]\n                )\n            fhand.flush()\n\n        array = chunk.gts.gt_ma_array\n        fpath = str(_create_gt_path(chunk_dir))\n        with gzip.open(\n            fpath,\n            mode=\"wb\",\n            compresslevel=numpy_array_compression_level,\n        ) as fhand:\n            numpy.save(fhand, array.data)\n            fhand.flush()\n        fpath = str(_create_gt_mask_path(chunk_dir))\n        with gzip.open(\n            fpath,\n            mode=\"wb\",\n            compresslevel=numpy_array_compression_level,\n        ) as fhand:\n            numpy.save(fhand, array.mask)\n            fhand.flush()\n\n        metadata[\"var_chunks_metadata\"].append(chunk_metadata)\n\n    with open(_create_metadata_path(output_dir), \"wt\") as fhand:\n        json.dump(metadata, fhand)\n        fhand.flush()\n\n\nclass VariantsDir:\n    def __init__(self, dir):\n        self.dir = Path(dir)\n        self.metadata = json.load(open(_create_metadata_path(self.dir), \"rt\"))\n        self.samples = numpy.array(self.metadata[\"samples\"])\n        self.num_samples = self.metadata[\"num_samples\"]\n        self.ploidy = int(self.metadata[\"ploidy\"])\n        self._chunks_metadata = self.metadata[\"var_chunks_metadata\"]\n\n    def iter_vars_chunks(self):\n        for chunk_metadata in self._chunks_metadata:\n            chunk_kwargs = {}\n            chunk_dir = self.dir / chunk_metadata[\"dir\"]\n            path = _create_vars_info_path(chunk_dir)\n            if path.exists():\n                chunk_kwargs[\"vars_info\"] = pandas.read_parquet(path)\n\n            path = _create_gt_path(chunk_dir)\n            if path.exists():\n                gts = numpy.load(gzip.open(path, \"rb\"))\n                mask = numpy.load(gzip.open(_create_gt_mask_path(chunk_dir), \"rb\"))\n                gts = numpy.ma.masked_array(gts, mask)\n                gts = Genotypes(numpy.ma.array(gts))\n                chunk_kwargs[\"gts\"] = gts\n\n            yield VariantsChunk(**chunk_kwargs)\n", "type": "text"}, {"name": "pynei/io_vcf.py", "content": "from pathlib import Path\nimport array\nfrom enum import Enum\nimport gzip\nimport functools\nimport itertools\n\nimport numpy\nimport pandas\n\nfrom pynei.variants import Variants, VariantsChunk, Genotypes\nfrom pynei import config\nfrom pynei.config import (\n    MISSING_ALLELE,\n    MAX_ALLELE_NUMBER,\n    PYTHON_ARRAY_TYPE,\n    BYTE_SIZE_OF_INT,\n)\n\nVCF_SAMPLE_LINE_ITEMS = [\n    \"#CHROM\",\n    \"POS\",\n    \"ID\",\n    \"REF\",\n    \"ALT\",\n    \"QUAL\",\n    \"FILTER\",\n    \"INFO\",\n    \"FORMAT\",\n]\n\n\nclass _VCFKind(Enum):\n    VCF = \"vcf\"\n    GzippedVCF = \"GzippedVCF\"\n\n\ndef _guess_vcf_file_kind(path: Path):\n    is_gzipped = False\n    with path.open(\"rb\") as fhand:\n        start = fhand.read(2)\n        if start[:1] == b\"#\":\n            return _VCFKind.VCF\n        elif start[:2] == b\"\\x1f\\x8b\":\n            is_gzipped = True\n\n    if not is_gzipped:\n        raise ValueError(\n            \"Invalid VCF file, it does not start with # and its not gzipped\"\n        )\n\n    with gzip.open(path) as fhand:\n        start = fhand.read(1)\n        if start[:1] == b\"#\":\n            return _VCFKind.GzippedVCF\n    raise ValueError(\"Invalid VCF gzipped file, it does not start with #\")\n\n\ndef _parse_metadata(fhand):\n    metadata = {}\n    for line in fhand:\n        if line.startswith(b\"##\"):\n            pass\n        elif line.startswith(b\"#CHROM\"):\n            items = line.decode().strip().split(\"\\t\")\n            if items[:9] != VCF_SAMPLE_LINE_ITEMS:\n                raise ValueError(\n                    \"Invalid VCF file, it has an invalid sample line: {line.decode()}\"\n                )\n            metadata[\"samples\"] = items[9:]\n            break\n        else:\n            raise ValueError(\"Invalid VCF file, it has no header\")\n\n    metadata[\"samples\"] = numpy.array(metadata[\"samples\"])\n    num_samples = metadata[\"samples\"].size\n    metadata[\"num_samples\"] = num_samples\n    try:\n        var_line = next(fhand)\n    except StopIteration:\n        raise ValueError(\"Empty VCF file, it has no variants\")\n    var_ = _parse_var_line(var_line, num_samples, ploidy=None)\n    metadata[\"ploidy\"] = var_[\"gts\"].shape[1]\n\n    return metadata\n\n\ndef _open_vcf(fpath):\n    kind = _guess_vcf_file_kind(fpath)\n    if kind == _VCFKind.GzippedVCF:\n        fhand = gzip.open(fpath, mode=\"rb\")\n    else:\n        fhand = fpath.open(\"rb\")\n    return fhand\n\n\n@functools.lru_cache\ndef _parse_allele(allele):\n    if allele == b\".\":\n        return True, MISSING_ALLELE\n    else:\n        allele = int(allele)\n    if allele > MAX_ALLELE_NUMBER:\n        raise NotImplementedError(\n            f\"Only alleles up to {MAX_ALLELE_NUMBER} are implemented: {allele}\"\n        )\n    return False, allele\n\n\n_EXPECT_PHASED = False\n\n\n@functools.lru_cache\ndef _parse_gt(gt):\n    if _EXPECT_PHASED:\n        sep, other_sep = b\"|\", b\"/\"\n        is_phased = True\n    else:\n        sep, other_sep = b\"/\", b\"|\"\n        is_phased = False\n    try:\n        return is_phased, tuple(map(_parse_allele, gt.split(sep)))\n    except ValueError:\n        pass\n    is_phased = not is_phased\n    return is_phased, tuple(map(_parse_allele, gt.split(other_sep)))\n\n\n@functools.lru_cache\ndef _decode_chrom(chrom):\n    return chrom.decode()\n\n\n@functools.lru_cache\ndef _get_gt_fmt_idx(gt_fmt):\n    return gt_fmt.split(b\":\").index(b\"GT\")\n\n\n@functools.lru_cache\ndef _parse_qual(qual):\n    if qual == b\".\":\n        return numpy.nan\n    return float(qual)\n\n\n@functools.lru_cache\ndef _parse_id(id_):\n    if id_ == b\".\":\n        return None\n    return id_.decode()\n\n\ndef _parse_var_line(line, num_samples, ploidy=None):\n    fields = line.split(b\"\\t\")\n    ref = fields[3].decode()\n    alt = fields[4]\n    if alt != b\".\":\n        alt = alt.decode().split(\",\")\n        alleles = [ref] + alt\n    else:\n        alleles = [ref]\n\n    gt_fmt_idx = _get_gt_fmt_idx(fields[8])\n\n    if ploidy is None:\n        alleles = _parse_gt(fields[9].split(b\":\")[gt_fmt_idx])\n        ploidy = len(alleles)\n\n    ref_gt_str = b\"/\".join([b\"0\"] * ploidy)\n    gts = array.array(\n        PYTHON_ARRAY_TYPE, bytearray(num_samples * ploidy * BYTE_SIZE_OF_INT)\n    )\n    missing_mask = array.array(\"b\", bytearray(num_samples * ploidy))\n    sample_idx = 0\n    for gt_str in fields[9:]:\n        gt_str = gt_str.split(b\":\")[gt_fmt_idx]\n        if gt_str == ref_gt_str:\n            continue\n        for allele_idx, (is_missing, allele) in enumerate(_parse_gt(gt_str)[1]):\n            if is_missing:\n                missing_mask[sample_idx + allele_idx] = 1\n            if allele != 0:\n                gts[sample_idx + allele_idx] = allele\n        sample_idx += ploidy\n    gts = numpy.frombuffer(gts, dtype=config.GT_NUMPY_DTYPE).reshape(\n        num_samples, ploidy\n    )\n    missing_mask = (\n        numpy.frombuffer(missing_mask, dtype=numpy.int8)\n        .reshape(num_samples, ploidy)\n        .astype(bool)\n    )\n\n    return {\n        \"chrom\": _decode_chrom(fields[0]),\n        \"pos\": int(fields[1]),\n        \"alleles\": alleles,\n        \"id\": _parse_id(fields[2]),\n        \"qual\": _parse_qual(fields[5]),\n        \"gts\": gts,\n        \"missing_mask\": missing_mask,\n    }\n\n\ndef _read_vars(fhand, metadata):\n    for line in fhand:\n        if line.startswith(b\"#CHROM\"):\n            break\n\n    num_samples = len(metadata[\"samples\"])\n    parse_var_line = functools.partial(\n        _parse_var_line, num_samples=num_samples, ploidy=metadata[\"ploidy\"]\n    )\n    vars = map(parse_var_line, fhand)\n    return vars\n\n\ndef parse_vcf(vcf_path: Path):\n    fpath = Path(vcf_path)\n    fhand = _open_vcf(fpath)\n    metadata = _parse_metadata(fhand)\n\n    fhand = _open_vcf(fpath)\n    vars = _read_vars(fhand, metadata)\n\n    return {\"metadata\": metadata, \"vars\": vars, \"fhand\": fhand}\n\n\nclass _FromVCFChunkIterFactory:\n    def __init__(self, vcf_path):\n        self.vcf_path = vcf_path\n        res = parse_vcf(self.vcf_path)\n        self.metadata = res[\"metadata\"]\n        res[\"fhand\"].close()\n\n    def iter_vars_chunks(self):\n        res = parse_vcf(self.vcf_path)\n        fhand = res[\"fhand\"]\n        samples = self.metadata[\"samples\"]\n\n        vars_chunks = itertools.batched(res[\"vars\"], config.DEF_NUM_VARS_PER_CHUNK)\n        for vars_chunk in vars_chunks:\n            chroms = []\n            poss = []\n            ids = []\n            quals = []\n            alleles = []\n            gts = []\n            missing_masks = []\n            max_num_alleles = 0\n            for var in vars_chunk:\n                chroms.append(var[\"chrom\"])\n                poss.append(var[\"pos\"])\n                ids.append(var[\"id\"])\n                quals.append(var[\"qual\"])\n                alleles.append(var[\"alleles\"])\n                max_num_alleles = max(max_num_alleles, len(var[\"alleles\"]))\n                gts.append(var[\"gts\"])\n                missing_masks.append(var[\"missing_mask\"])\n            vars_info = pandas.DataFrame(\n                {\n                    config.VAR_TABLE_CHROM_COL: pandas.Series(\n                        chroms, dtype=config.PANDAS_STR_DTYPE()\n                    ),\n                    config.VAR_TABLE_POS_COL: pandas.Series(\n                        poss, dtype=config.PANDAS_INT_DTYPE()\n                    ),\n                    config.VAR_TABLE_ID_COL: pandas.Series(\n                        ids, dtype=config.PANDAS_STR_DTYPE()\n                    ),\n                    config.VAR_TABLE_QUAL_COL: pandas.Series(\n                        quals, dtype=config.PANDAS_FLOAT_DTYPE()\n                    ),\n                },\n            )\n            alleles = pandas.DataFrame(alleles, dtype=config.PANDAS_STR_DTYPE())\n            gts = numpy.ma.array(\n                gts, mask=missing_masks, fill_value=config.MISSING_ALLELE\n            )\n            gts.flags.writeable = False\n            gts = Genotypes(gts, samples=samples, skip_mask_check=True)\n            chunk = VariantsChunk(gts=gts, vars_info=vars_info, alleles=alleles)\n            yield chunk\n        fhand.close()\n\n    def _get_metadata(self):\n        return self.metadata\n\n\ndef vars_from_vcf(vcf_path: Path) -> Variants:\n    chunk_factory = _FromVCFChunkIterFactory(vcf_path)\n    vars = Variants(chunk_factory)\n\n    return vars\n", "type": "text"}, {"name": "pynei/ld.py", "content": "import itertools\nfrom functools import partial\nfrom collections import namedtuple\nfrom typing import Sequence\nfrom enum import Enum\n\nimport numpy\nimport pandas\nimport more_itertools\n\nfrom pynei.config import VAR_TABLE_CHROM_COL, VAR_TABLE_POS_COL, DEF_POP_NAME\nfrom pynei.var_filters import filter_by_maf, filter_samples\n\nDDOF = 1\n\n\ndef _calc_maf_from_012_gts(gts: numpy.array):\n    counts = {}\n    for gt in [0, 1, 2]:\n        counts[gt] = numpy.sum(gts == gt, axis=1)\n\n    num_gts_per_var = counts[0] + counts[1] + counts[2]\n    counts_major_allele = numpy.maximum(counts[0], counts[2])\n    freqs_major_allele = counts_major_allele / num_gts_per_var\n    freqs_het = counts[1] / num_gts_per_var\n    maf_per_var = freqs_major_allele + 0.5 * freqs_het\n    return maf_per_var\n\n\ndef _calc_rogers_huff_r2(\n    gts1: numpy.ndarray,\n    gts2: numpy.ndarray,\n    check_no_mafs_above: float | None = 0.95,\n    debug=False,\n):\n    if check_no_mafs_above is not None:\n        maf_per_var = _calc_maf_from_012_gts(gts1)\n        if numpy.any(maf_per_var > check_no_mafs_above):\n            raise ValueError(\n                f\"There are variations with mafs above {check_no_mafs_above}, filter them out or modify this check\"\n            )\n\n    covars = numpy.cov(gts1, gts2, ddof=DDOF)\n    n_vars1 = gts1.shape[0]\n    n_vars2 = gts2.shape[0]\n    if debug:\n        print(\"nvars\", n_vars1, n_vars2)\n    variances = numpy.diag(covars)\n    vars1 = variances[:n_vars1]\n    vars2 = variances[n_vars1:]\n    if debug:\n        print(\"vars1\", vars1)\n        print(\"vars2\", vars2)\n\n    covars = covars[:n_vars1, n_vars1:]\n    if debug:\n        print(\"covars\", covars)\n\n    vars1 = numpy.repeat(vars1, n_vars2).reshape((n_vars1, n_vars2))\n    vars2 = numpy.tile(vars2, n_vars1).reshape((n_vars1, n_vars2))\n    with numpy.errstate(divide=\"ignore\", invalid=\"ignore\"):\n        rogers_huff_r = covars / numpy.sqrt(vars1 * vars2)\n    # print(vars1)\n    # print(vars2)\n    return rogers_huff_r\n\n\ndef _chunks_are_close(chunk_pair, max_dist):\n    chunk1 = chunk_pair[0]\n    chunk2 = chunk_pair[1]\n    common_chroms = numpy.intersect1d(\n        chunk1.vars_info[VAR_TABLE_CHROM_COL].values,\n        chunk2.vars_info[VAR_TABLE_CHROM_COL].values,\n    )\n    if not common_chroms.size:\n        return False\n\n    chroms1 = chunk1.vars_info[VAR_TABLE_CHROM_COL]\n    chroms2 = chunk2.vars_info[VAR_TABLE_CHROM_COL]\n    poss1 = chunk1.vars_info[VAR_TABLE_POS_COL]\n    poss2 = chunk2.vars_info[VAR_TABLE_POS_COL]\n    for chrom in common_chroms:\n        chrom_poss1 = poss1[chroms1 == chrom]\n        chrom_poss2 = poss2[chroms2 == chrom]\n        poss1_start = chrom_poss1.iloc[0]\n        poss1_end = chrom_poss1.iloc[-1]\n        poss2_start = chrom_poss2.iloc[0]\n        poss2_end = chrom_poss2.iloc[-1]\n        if poss1_start == poss2_start or poss1_end == poss2_end:\n            return True\n        dist = poss2_start - poss1_end\n        if dist <= max_dist:\n            return True\n\n    return False\n\n\ndef calc_rogers_huff_r2_matrix(\n    vars, max_dist: int | None = None, check_no_mafs_above: float | None = 0.95\n):\n    # This function is faster than calc_pairwise_rogers_huff_r2,\n    # but it uses much more memory\n    chunks = list(vars.iter_vars_chunks())\n    tot_num_vars = sum(chunk.num_vars for chunk in chunks)\n    r2 = numpy.full((tot_num_vars, tot_num_vars), numpy.nan)\n    res = {\"r2\": r2}\n    dists = None\n    row_start = 0\n    for chunk1 in chunks:\n        col_start = 0\n        for chunk2 in chunks:\n            if max_dist:\n                if not _chunks_are_close((chunk1, chunk2), max_dist):\n                    continue\n            row_end = row_start + chunk1.num_vars\n            col_end = col_start + chunk2.num_vars\n\n            this_r2 = _calc_rogers_huff_r2(\n                chunk1.gts.to_012(),\n                chunk2.gts.to_012(),\n                check_no_mafs_above=check_no_mafs_above,\n            )\n            r2[row_start:row_end, col_start:col_end] = this_r2\n\n            chroms1, poss1, chroms2, poss2 = None, None, None, None\n            if chunk1.vars_info is not None:\n                try:\n                    chroms1 = chunk1.vars_info[VAR_TABLE_CHROM_COL]\n                except KeyError:\n                    pass\n                try:\n                    poss1 = chunk1.vars_info[VAR_TABLE_POS_COL].to_numpy()\n                except KeyError:\n                    pass\n                try:\n                    chroms2 = chunk2.vars_info[VAR_TABLE_CHROM_COL]\n                except KeyError:\n                    pass\n                try:\n                    poss2 = chunk2.vars_info[VAR_TABLE_POS_COL].to_numpy()\n                except KeyError:\n                    pass\n                if not any(\n                    [\n                        chroms1 is None,\n                        chroms2 is None,\n                        poss1 is None,\n                        poss2 is None,\n                    ]\n                ):\n                    mat1 = numpy.repeat(poss1, chunk2.num_vars).reshape(\n                        (chunk1.num_vars, chunk2.num_vars)\n                    )\n                    mat2 = numpy.tile(poss2, chunk1.num_vars).reshape(\n                        (chunk1.num_vars, chunk2.num_vars)\n                    )\n                    this_dists = numpy.abs(mat1 - mat2).astype(float)\n\n                    chroms = pandas.concat([chroms1, chroms2])\n                    chroms = pandas.factorize(chroms)[0]\n                    chroms1 = chroms[: chroms1.size]\n                    chroms2 = chroms[chroms2.size :]\n                    mat1 = numpy.repeat(chroms1, chunk2.num_vars).reshape(\n                        (chunk1.num_vars, chunk2.num_vars)\n                    )\n                    mat2 = numpy.tile(chroms2, chunk1.num_vars).reshape(\n                        (chunk1.num_vars, chunk2.num_vars)\n                    )\n                    is_different_chrom = mat1 != mat2\n                    this_dists[is_different_chrom] = numpy.nan\n\n                    if dists is None:\n                        dists = numpy.full((tot_num_vars, tot_num_vars), numpy.nan)\n                        res[\"dists_in_bp\"] = dists\n                    dists[row_start:row_end, col_start:col_end] = this_dists\n\n            col_start = col_end\n        row_start = row_end\n\n    return res\n\n\nLDResult = namedtuple(\n    \"LDResult\", [\"r2\", \"chrom_var1\", \"pos_var1\", \"chrom_var2\", \"pos_var2\", \"dist_in_bp\"]\n)\n\n\ndef calc_pairwise_rogers_huff_r2(\n    vars, max_dist: int | None = None, check_no_mafs_above: float | None = 0.95\n):\n    # This is the slower alternative, calc_rogers_huff_r2_matrix is much faster,\n    # but if you have many vars and the calculation does not fit in memory, use this one\n    chunks = vars.iter_vars_chunks()\n    chunk_pairs = itertools.combinations_with_replacement(chunks, 2)\n\n    if max_dist is not None:\n        chunks_are_close = partial(_chunks_are_close, max_dist=max_dist)\n        chunk_pairs = filter(chunks_are_close, chunk_pairs)\n\n    for chunk1, chunk2 in chunk_pairs:\n        r2 = _calc_rogers_huff_r2(\n            chunk1.gts.to_012(),\n            chunk2.gts.to_012(),\n            check_no_mafs_above=check_no_mafs_above,\n        )\n\n        poss1, poss2, chroms1, chroms2 = None, None, None, None\n        # print(chunk1.vars_info)\n        if chunk1.vars_info is not None and chunk2.vars_info is not None:\n            try:\n                poss1 = numpy.array(chunk1.vars_info[VAR_TABLE_POS_COL].values)\n            except KeyError:\n                pass\n            try:\n                poss2 = numpy.array(chunk2.vars_info[VAR_TABLE_POS_COL].values)\n            except KeyError:\n                pass\n            try:\n                chroms1 = numpy.array(chunk1.vars_info[VAR_TABLE_CHROM_COL].values)\n            except KeyError:\n                pass\n            try:\n                chroms2 = numpy.array(chunk2.vars_info[VAR_TABLE_CHROM_COL].values)\n            except KeyError:\n                pass\n\n        both_chunks_are_same = chunk1 is chunk2\n        for idx1 in range(chunk1.num_vars):\n            for idx2 in range(chunk2.num_vars):\n                if both_chunks_are_same and idx1 >= idx2:\n                    continue\n                if (\n                    chroms1 is not None\n                    and chroms2 is not None\n                    and poss1 is not None\n                    and poss2 is not None\n                ):\n                    pos1 = int(poss1[idx1])\n                    pos2 = int(poss2[idx2])\n                    chrom1 = chroms1[idx1]\n                    chrom2 = chroms2[idx2]\n                    dist = abs(pos1 - pos2) if chrom1 == chrom2 else None\n                else:\n                    pos1, pos2, chrom1, chrom2, dist = None, None, None, None, None\n\n                if max_dist is not None:\n                    if dist is None or dist > max_dist:\n                        continue\n\n                pair_r2 = float(r2[idx1, idx2])\n                yield LDResult(pair_r2, chrom1, pos1, chrom2, pos2, dist)\n\n\nclass LDCalcMethod(Enum):\n    GENERATOR = \"generator\"\n    MATRIX = \"matrix\"\n\n\ndef get_ld_and_dist_for_pops(\n    vars,\n    pops: dict[str, Sequence[str] | Sequence[int]] | None = None,\n    max_dist: int | None = None,\n    min_dist: int | None = 1,\n    max_allowed_maf=0.95,\n    method=LDCalcMethod.GENERATOR,\n    max_num_measures_to_keep=10000,\n):\n    if pops is None:\n        pops = {DEF_POP_NAME: slice(None, None)}\n\n    ld_per_pop = {}\n    for pop_name, samples in pops.items():\n        pop_vars = filter_samples(vars, samples)\n        pop_vars = filter_by_maf(pop_vars, max_allowed_maf=max_allowed_maf)\n        if method == LDCalcMethod.GENERATOR:\n            lds_and_dists = (\n                (res.r2, res.dist_in_bp)\n                for res in calc_pairwise_rogers_huff_r2(\n                    pop_vars, max_dist=max_dist, check_no_mafs_above=None\n                )\n                if res.dist_in_bp is not None\n            )\n        elif method == LDCalcMethod.MATRIX:\n            res = calc_rogers_huff_r2_matrix(\n                pop_vars, max_dist=max_dist, check_no_mafs_above=None\n            )\n            r2 = res[\"r2\"].flat\n            dists = res[\"dists_in_bp\"].flat\n            mask = ~numpy.isnan(dists)\n            r2 = r2[mask]\n            dists = dists[mask]\n            lds_and_dists = [(float(r2), float(ld)) for r2, ld in zip(r2, dists)]\n\n        if min_dist:\n            lds_and_dists = filter(lambda x: x[1] > min_dist, lds_and_dists)\n        try:\n            lds_and_dists = more_itertools.sample(\n                lds_and_dists, k=max_num_measures_to_keep, strict=False\n            )\n        except TypeError:\n            # old versions of more-itertools.sample seem to lack the strict argument\n            lds_and_dists = more_itertools.sample(\n                lds_and_dists,\n                k=max_num_measures_to_keep,\n            )\n        ld_per_pop[pop_name] = lds_and_dists\n    return ld_per_pop\n\n\n# calc_ld_along_genome()\n# filter_vars_by_ld\n", "type": "text"}, {"name": "pynei/pca.py", "content": "from functools import partial\n\nimport numpy\nimport pandas\n\nfrom pynei.gt_counts import _count_alleles_per_var\nfrom pynei.config import DEF_POP_NAME\nfrom pynei.pipeline import Pipeline\nfrom pynei.dists import Distances, calc_pairwise_kosman_dists\n\n\ndef _create_012_gt_matrix(chunk, transform_to_biallelic=False):\n    res = _count_alleles_per_var(chunk, calc_freqs=False)\n    allele_counts = res[\"counts\"][DEF_POP_NAME][\"allele_counts\"].values\n\n    num_genotyped_alleles_per_var = allele_counts.sum(axis=1)\n    if numpy.any(num_genotyped_alleles_per_var == 0):\n        raise ValueError(\"There are variants that only have missing data\")\n\n    max_num_alleles = allele_counts.shape[1]\n    if max_num_alleles > 2 and not transform_to_biallelic:\n        raise ValueError(\n            f\"In order to get the 012 matrix you should pass transform_to_biallelic=True, because you have {max_num_alleles} in at least one variation\"\n        )\n\n    major_alleles = numpy.argmax(allele_counts, axis=1)\n    gt_array = chunk.gts.gt_values\n    gts012 = numpy.sum(gt_array != major_alleles[:, None, None], axis=2)\n    return gts012\n\n\ndef _append_array(array: numpy.ndarray | None, array_to_append: numpy.ndarray):\n    if array is None:\n        return array_to_append\n\n    return numpy.vstack((array, array_to_append))\n\n\ndef create_012_gt_matrix(variants, transform_to_biallelic=False):\n    create_012_matrix = partial(\n        _create_012_gt_matrix, transform_to_biallelic=transform_to_biallelic\n    )\n    pipeline = Pipeline(map_functs=[create_012_matrix], reduce_funct=_append_array)\n    return pipeline.map_and_reduce(variants)\n\n\ndef _create_pc_names(num_prin_comps):\n    n_digits = num_prin_comps // 10\n    fstring = \"{:0\" + str(n_digits) + \"d}\"\n    prin_comps_names = [\"PC\" + fstring.format(idx) for idx in range(num_prin_comps)]\n    return prin_comps_names\n\n\ndef do_pca(data: pandas.DataFrame, center_data=True, standarize_data=True):\n    if numpy.any(numpy.isnan(data)):\n        raise ValueError(\"data can have no nan values\")\n\n    if standarize_data and not center_data:\n        raise ValueError(\"If you standarize you have to also center the data\")\n\n    trait_names = data.columns\n    sample_names = data.index\n\n    data = data.values\n    num_samples, num_traits = data.shape\n\n    if center_data:\n        data = data - data.mean(axis=0)\n\n    if standarize_data:\n        data = data / data.std(axis=0)\n\n    U, Sigma, Vh = numpy.linalg.svd(data, full_matrices=False)\n    singular_vals = Sigma\n    prin_comps = Vh\n    num_prin_comps = prin_comps.shape[0]\n    prin_comps_names = _create_pc_names(num_prin_comps)\n\n    eig_vals = numpy.square(singular_vals) / (num_samples - 1)\n    pcnts = eig_vals / eig_vals.sum() * 100.0\n    projections = numpy.dot(prin_comps, data.T).T\n\n    return {\n        \"projections\": pandas.DataFrame(\n            projections, index=sample_names, columns=prin_comps_names\n        ),\n        \"explained_variance (%)\": pandas.Series(pcnts, index=prin_comps_names),\n        \"princomps\": pandas.DataFrame(\n            prin_comps, index=prin_comps_names, columns=trait_names\n        ),\n    }\n\n\ndef do_pca_with_vars(variants, transform_to_biallelic=False):\n    mat012 = create_012_gt_matrix(\n        variants, transform_to_biallelic=transform_to_biallelic\n    )\n    mat012 = pandas.DataFrame(mat012.T, index=variants.samples)\n    return do_pca(mat012, center_data=True, standarize_data=True)\n\n\ndef _make_f_matrix(matrix):\n    \"\"\"It takes an E matrix and returns an F matrix\n\n    The input is the output of make_E_matrix\n\n    For each element in matrix subtract mean of corresponding row and\n    column and add the mean of all elements in the matrix\n    \"\"\"\n    num_rows, num_cols = matrix.shape\n    # make a vector of the means for each row and column\n    # column_means = (numpy.add.reduce(E_matrix) / num_rows)\n    column_means = (numpy.add.reduce(matrix) / num_rows)[:, numpy.newaxis]\n    trans_matrix = numpy.transpose(matrix)\n    row_sums = numpy.add.reduce(trans_matrix)\n    row_means = row_sums / num_cols\n    # calculate the mean of the whole matrix\n    matrix_mean = numpy.sum(row_sums) / (num_rows * num_cols)\n    # adjust each element in the E matrix to make the F matrix\n\n    matrix -= row_means\n    matrix -= column_means\n    matrix += matrix_mean\n\n    return matrix\n\n\ndef do_pcoa(dists: Distances):\n    \"It does a Principal Coordinate Analysis on a distance matrix\"\n    # the code for this function is taken from pycogent metric_scaling.py\n    # Principles of Multivariate analysis: A User's Perspective.\n    # W.J. Krzanowski Oxford University Press, 2000. p106.\n\n    sample_names = dists.names\n    dists = dists.square_dists.values\n\n    if numpy.any(numpy.isnan(dists)):\n        raise ValueError(\"dists array has nan values\")\n\n    e_matrix = (dists * dists) / -2.0\n    f_matrix = _make_f_matrix(e_matrix)\n\n    eigvals, eigvecs = numpy.linalg.eigh(f_matrix)\n    eigvecs = eigvecs.transpose()\n    # drop imaginary component, if we got one\n    eigvals, eigvecs = eigvals.real, eigvecs.real\n\n    # convert eigvals and eigvecs to point matrix\n    # normalized eigenvectors with eigenvalues\n\n    # get the coordinates of the n points on the jth axis of the Euclidean\n    # representation as the elements of (sqrt(eigvalj))eigvecj\n    # must take the absolute value of the eigvals since they can be negative\n    pca_matrix = eigvecs * numpy.sqrt(abs(eigvals))[:, numpy.newaxis]\n\n    # output\n    # get order to output eigenvectors values. reports the eigvecs according\n    # to their cooresponding eigvals from greatest to least\n    vector_order = list(numpy.argsort(eigvals))\n    vector_order.reverse()\n\n    eigvals = eigvals[vector_order]\n\n    # eigenvalues\n    pcnts = (eigvals / numpy.sum(eigvals)) * 100.0\n\n    # the outputs\n    # eigenvectors in the original pycogent implementation, here we name them\n    # princoords\n    # I think that we're doing: if the eigenvectors are written as columns,\n    # the rows of the resulting table are the coordinates of the objects in\n    # PCO space\n    projections = []\n    for name_i in range(dists.shape[0]):\n        eigvect = [pca_matrix[vec_i, name_i] for vec_i in vector_order]\n        projections.append(eigvect)\n    projections = numpy.array(projections)\n    prin_comps_names = _create_pc_names(projections.shape[1])\n\n    return {\n        \"projections\": pandas.DataFrame(\n            projections, index=sample_names, columns=prin_comps_names\n        ),\n        \"explained_variance (%)\": pandas.Series(pcnts, index=prin_comps_names),\n    }\n\n\ndef do_pcoa_with_vars(\n    variants, min_num_snps=None, use_approx_embedding_algorithm=False\n):\n    dists = calc_pairwise_kosman_dists(\n        variants,\n        min_num_snps=min_num_snps,\n        use_approx_embedding_algorithm=use_approx_embedding_algorithm,\n    )\n    return do_pcoa(dists)\n", "type": "text"}, {"name": "pynei/pipeline.py", "content": "from typing import Callable, Iterator\nimport multiprocessing\nimport functools\n\n\nclass _ChunkProcessor:\n    def __init__(self, map_functs):\n        self.map_functs = map_functs\n\n    def __call__(self, item):\n        processed_item = item\n        for one_funct in self.map_functs:\n            processed_item = one_funct(processed_item)\n        return processed_item\n\n\nclass Pipeline:\n    def __init__(\n        self,\n        map_functs: list[Callable] | None = None,\n        reduce_funct: Callable = None,\n        reduce_initializer=None,\n        after_reduce_funct: Callable = None,\n    ):\n        self.map_functs = map_functs\n        self.reduce_funct = reduce_funct\n        self.reduce_initializer = reduce_initializer\n        self.after_reduce_funct = after_reduce_funct\n\n    def append_map_funct(self, map_funct: Callable):\n        self.map_functs.append(map_funct)\n\n    def set_reduce_funct(self, reduce_funct: Callable, reduce_initializer=None):\n        self.reduce_funct = reduce_funct\n        self.reduce_initializer = reduce_initializer\n\n    def _process_vars(self, vars, num_processes: int = 1):\n        process_chunk = _ChunkProcessor(self.map_functs)\n\n        use_multiprocessing = num_processes > 1\n\n        if use_multiprocessing:\n            pool = multiprocessing.Pool(3)\n            map_ = pool.map\n        else:\n            map_ = map\n\n        processed_chunks = map_(process_chunk, vars.iter_vars_chunks())\n\n        if self.reduce_funct is not None:\n            reduced_result = functools.reduce(\n                self.reduce_funct, processed_chunks, self.reduce_initializer\n            )\n            result = reduced_result\n        else:\n            result = processed_chunks\n\n        if use_multiprocessing:\n            pool.close()\n\n        if self.after_reduce_funct is not None:\n            result = self.after_reduce_funct(result)\n\n        return result\n\n    def map_chunks(self, vars, num_processes: int = 1) -> Iterator:\n        if self.reduce_funct is not None or self.reduce_initializer is not None:\n            raise ValueError(\n                \"For mapping reduce_funct and reduce_initializer must be None\"\n            )\n        return self._process_vars(vars, num_processes)\n\n    def map_and_reduce(self, vars, num_processes: int = 1):\n        if self.reduce_funct is None:\n            raise ValueError(\"For mapping and reducing reduce_funct must be set\")\n        return self._process_vars(vars, num_processes)\n", "type": "text"}, {"name": "pynei/utils_pop.py", "content": "from pynei.config import DEF_POP_NAME\n\n\ndef _calc_pops_idxs(pops: dict[list[str]] | None, samples):\n    if pops is None:\n        pops_idxs = {DEF_POP_NAME: slice(None, None)}\n    else:\n        if samples is None:\n            raise ValueError(\"Variants should have samples defined if pops is not None\")\n        samples_idx = {sample: idx for idx, sample in enumerate(samples)}\n        pops_idxs = {}\n        for pop_id, pop_samples in pops.items():\n            pops_idxs[pop_id] = [samples_idx[sample] for sample in pop_samples]\n    return pops_idxs\n", "type": "text"}, {"name": "pynei/utils_stats.py", "content": "from functools import partial\n\nimport numpy\nimport pandas\n\nfrom pynei.config import (\n    BinType,\n    LINEAL,\n    LOGARITHMIC,\n)\nfrom pynei.pipeline import Pipeline\n\n\ndef _prepare_bins(\n    hist_kwargs: dict,\n    range=tuple[int, int],\n    default_num_bins=40,\n    default_bin_type: BinType = LINEAL,\n):\n    num_bins = hist_kwargs.get(\"num_bins\", default_num_bins)\n    bin_type = hist_kwargs.get(\"bin_type\", default_bin_type)\n\n    if bin_type == LINEAL:\n        bins = numpy.linspace(range[0], range[1], num_bins + 1)\n    elif bin_type == LOGARITHMIC:\n        if range[0] == 0:\n            raise ValueError(\"range[0] cannot be zero for logarithmic bins\")\n        bins = numpy.logspace(range[0], range[1], num_bins + 1)\n    return bins\n\n\ndef _collect_stats_from_pop_dframes(\n    accumulated_result, next_result: pandas.DataFrame, hist_bins_edges: numpy.array\n):\n    if accumulated_result is None:\n        accumulated_result = {\n            \"sum_per_pop\": pandas.Series(\n                numpy.zeros((next_result.shape[1]), dtype=int),\n                index=next_result.columns,\n            ),\n            \"total_num_rows\": pandas.Series(\n                numpy.zeros((next_result.shape[1]), dtype=int),\n                index=next_result.columns,\n            ),\n            \"hist_counts\": None,\n        }\n\n    accumulated_result[\"sum_per_pop\"] += next_result.sum(axis=0)\n    accumulated_result[\"total_num_rows\"] += next_result.shape[\n        0\n    ] - next_result.isna().sum(axis=0)\n\n    this_counts = {}\n    for pop, pop_stats in next_result.items():\n        this_counts[pop] = numpy.histogram(pop_stats, bins=hist_bins_edges)[0]\n    this_counts = pandas.DataFrame(this_counts)\n\n    if accumulated_result[\"hist_counts\"] is None:\n        accumulated_result[\"hist_counts\"] = this_counts\n    else:\n        accumulated_result[\"hist_counts\"] += this_counts\n\n    return accumulated_result\n\n\ndef _calc_stats_per_var(\n    variants,\n    calc_stats_for_chunk,\n    get_stats_for_chunk_result,\n    hist_kwargs=None,\n):\n    if hist_kwargs is None:\n        hist_kwargs = {}\n    hist_bins_edges = _prepare_bins(hist_kwargs, range=hist_kwargs[\"range\"])\n\n    collect_stats_from_pop_dframes = partial(\n        _collect_stats_from_pop_dframes, hist_bins_edges=hist_bins_edges\n    )\n\n    pipeline = Pipeline(\n        map_functs=[\n            calc_stats_for_chunk,\n            get_stats_for_chunk_result,\n        ],\n        reduce_funct=collect_stats_from_pop_dframes,\n    )\n    accumulated_result = pipeline.map_and_reduce(variants)\n\n    mean = accumulated_result[\"sum_per_pop\"] / accumulated_result[\"total_num_rows\"]\n    return {\n        \"mean\": mean,\n        \"hist_bin_edges\": hist_bins_edges,\n        \"hist_counts\": accumulated_result[\"hist_counts\"],\n    }\n", "type": "text"}, {"name": "pynei/var_filters.py", "content": "from functools import partial\nimport itertools\nfrom typing import Sequence\n\nimport numpy\n\nfrom pynei.variants import Variants, VariantsChunk\nfrom pynei.gt_counts import (\n    _calc_gt_is_missing,\n    _calc_maf_per_var,\n    _calc_obs_het_per_var,\n)\n\n\nclass _FilterChunkIterFactory:\n    def __init__(self, in_vars, filter_funct):\n        self.in_vars = in_vars\n        self._chunks = in_vars.iter_vars_chunks()\n        self.filter_funct = filter_funct\n        self.num_vars_processed = 0\n        self.num_vars_kept = 0\n        self._metadata = None\n\n    def _get_metadata(self):\n        if self._metadata is not None:\n            return self._metadata.copy()\n\n        try:\n            first_chunk = next(self.iter_vars_chunks())\n        except StopIteration:\n            raise RuntimeError(\"No variations to get the data from\")\n\n        self._chunks = itertools.chain([first_chunk], self._chunks)\n\n        self._metadata = {\n            \"samples\": first_chunk.gts.samples,\n            \"num_samples\": first_chunk.num_samples,\n            \"ploidy\": first_chunk.gts.ploidy,\n        }\n        return self._metadata.copy()\n\n    def iter_vars_chunks(self):\n        for chunk in self._chunks:\n            if self._metadata is None:\n                self._metadata = {\n                    \"samples\": chunk.gts.samples,\n                    \"num_samples\": chunk.num_samples,\n                    \"ploidy\": chunk.gts.ploidy,\n                }\n            filtered_chunk, num_vars_kept = self.filter_funct(chunk)\n            self.num_vars_processed += chunk.num_vars\n            self.num_vars_kept += num_vars_kept\n            yield filtered_chunk\n\n\nclass _MissingFilterIterFactory(_FilterChunkIterFactory):\n    kind = \"missing_data\"\n\n\ndef gather_filtering_stats(vars: Variants, stats=None):\n    if stats is None:\n        stats = {}  # stats by filter kind\n    chunk_factory = vars._vars_chunks_iter_factory\n    if isinstance(chunk_factory, _FilterChunkIterFactory):\n        filter_kind = chunk_factory.kind\n        if filter_kind not in stats:\n            stats[filter_kind] = {\"vars_processed\": 0, \"vars_kept\": 0}\n        stats[filter_kind][\"vars_processed\"] += chunk_factory.num_vars_processed\n        stats[filter_kind][\"vars_kept\"] += chunk_factory.num_vars_kept\n    if hasattr(chunk_factory, \"in_vars\"):\n        gather_filtering_stats(chunk_factory.in_vars, stats)\n\n    for filtering_stats in stats.values():\n        if \"vars_kept\" in filtering_stats and hasattr(\n            filtering_stats[\"vars_kept\"], \"item\"\n        ):\n            # this is to convert from np.int64 to native python int\n            filtering_stats[\"vars_kept\"] = filtering_stats[\"vars_kept\"].item()\n    return stats\n\n\ndef _filter_chunk_by_missing(chunk, max_missing_rate):\n    num_missing_per_var = numpy.sum(_calc_gt_is_missing(chunk)[\"gt_is_missing\"], axis=1)\n    missing_rate_per_var = num_missing_per_var / chunk.num_samples\n    mask = missing_rate_per_var <= max_missing_rate\n    num_vars_kept = mask.sum()\n    chunk = chunk.get_vars(mask)\n    return chunk, num_vars_kept\n\n\ndef filter_by_missing_data(\n    vars: Variants, max_allowed_missing_rate: float = 0.0\n) -> Variants:\n    filter_chunk_by_missing = partial(\n        _filter_chunk_by_missing, max_missing_rate=max_allowed_missing_rate\n    )\n    chunk_factory = _MissingFilterIterFactory(vars, filter_chunk_by_missing)\n    return Variants(\n        vars_chunk_iter_factory=chunk_factory,\n        desired_num_vars_per_chunk=vars.desired_num_vars_per_chunk,\n    )\n\n\nclass _MafFilterIterFactory(_FilterChunkIterFactory):\n    kind = \"maf\"\n\n\ndef _filter_chunk_by_maf(chunk, max_allowed_maf):\n    mafs = _calc_maf_per_var(chunk, pops={\"pop\": slice(None, None)}, min_num_samples=0)[\n        \"major_allele_freqs_per_var\"\n    ][\"pop\"]\n    mask = mafs <= max_allowed_maf\n    num_vars_kept = mask.sum()\n    chunk = chunk.get_vars(mask)\n    return chunk, num_vars_kept\n\n\ndef filter_by_maf(vars: Variants, max_allowed_maf) -> Variants:\n    filter_chunk = partial(_filter_chunk_by_maf, max_allowed_maf=max_allowed_maf)\n    chunk_factory = _MafFilterIterFactory(vars, filter_chunk)\n    return Variants(\n        vars_chunk_iter_factory=chunk_factory,\n        desired_num_vars_per_chunk=vars.desired_num_vars_per_chunk,\n    )\n\n\nclass _ObsHetFilterIterFactory(_FilterChunkIterFactory):\n    kind = \"obs_het\"\n\n\ndef _filter_chunk_by_obs_het(chunk, max_allowed_obs_het):\n    obs_hets = _calc_obs_het_per_var(chunk, pops={\"pop\": slice(None, None)})[\n        \"obs_het_per_var\"\n    ][\"pop\"]\n    mask = obs_hets <= max_allowed_obs_het\n    num_vars_kept = mask.sum()\n    chunk = chunk.get_vars(mask)\n    return chunk, num_vars_kept\n\n\ndef filter_by_obs_het(vars: Variants, max_allowed_obs_het: float):\n    filter_chunk = partial(\n        _filter_chunk_by_obs_het, max_allowed_obs_het=max_allowed_obs_het\n    )\n    chunk_factory = _ObsHetFilterIterFactory(vars, filter_chunk)\n    return Variants(\n        vars_chunk_iter_factory=chunk_factory,\n        desired_num_vars_per_chunk=vars.desired_num_vars_per_chunk,\n    )\n\n\ndef _filter_samples(chunk, sample_idxs):\n    gts = chunk.gts.filter_samples_with_idxs(sample_idxs)\n    chunk = VariantsChunk(gts, vars_info=chunk.vars_info, alleles=chunk.alleles)\n    return chunk, chunk.num_vars\n\n\nclass _SampleFilterIterFactory(_FilterChunkIterFactory):\n    kind = \"sample\"\n\n\ndef filter_samples(vars, samples: Sequence[str] | Sequence[int] | slice) -> Variants:\n    orig_samples = vars.samples\n    if isinstance(samples, slice):\n        samples = orig_samples[samples]\n    sample_idxs = numpy.where(numpy.isin(orig_samples, samples))[0]\n\n    filter_samples = partial(_filter_samples, sample_idxs=sample_idxs)\n    chunk_factory = _SampleFilterIterFactory(vars, filter_samples)\n    return Variants(\n        vars_chunk_iter_factory=chunk_factory,\n        desired_num_vars_per_chunk=vars.desired_num_vars_per_chunk,\n    )\n\n\n# TODO\n# var QUAL\n", "type": "text"}, {"name": "pynei/variants.py", "content": "from typing import Iterator, Self, Sequence, Protocol\nfrom collections.abc import Sequence as SequenceABC\n\nimport numpy\nimport pandas\n\nfrom .config import (\n    PANDAS_STRING_STORAGE,\n    PANDAS_FLOAT_DTYPE,\n    PANDAS_INT_DTYPE,\n    DEF_NUM_VARS_PER_CHUNK,\n    MISSING_ALLELE,\n    DEF_POP_NAME,\n)\nfrom pynei.gt_counts import _count_alleles_per_var\n\n\nclass Genotypes:\n    def __init__(\n        self,\n        gt_array: numpy.ma.masked_array,\n        samples: numpy.ndarray | Sequence[str] | None = None,\n        skip_mask_check=False,\n    ):\n        if not numpy.ma.isarray(gt_array):\n            mask = gt_array == MISSING_ALLELE\n            gt_array = numpy.ma.array(gt_array, mask=mask)\n            skip_mask_check = True\n\n        if not numpy.issubdtype(gt_array.dtype, numpy.integer):\n            raise ValueError(\"gts must be an integer numpy array\")\n        if not gt_array.ndim == 3:\n            raise ValueError(\"gts must be a 3D numpy array: vars x samples x ploidy\")\n\n        if gt_array.flags.writeable:\n            gt_array = gt_array.copy()\n            gt_array.flags.writeable = False\n\n        if not skip_mask_check:\n            if not numpy.array_equal(\n                numpy.ma.getdata(gt_array) == MISSING_ALLELE,\n                numpy.ma.getmaskarray(gt_array),\n            ):\n                raise ValueError(\n                    f\"Missing values should be {MISSING_ALLELE} in the values and masked, but {MISSING_ALLELE} and mask do not match\"\n                )\n\n        if samples is not None:\n            samples = numpy.array(samples)\n            samples.flags.writeable = False\n            if len(set(samples)) < samples.size:\n                unique_elements, counts = numpy.unique(samples, return_counts=True)\n                duplicated_samples = unique_elements[counts > 1]\n                raise ValueError(f\"Duplicated sample names: {duplicated_samples}\")\n            samples.flags.writeable = False\n\n            if gt_array.shape[1] != samples.size:\n                raise ValueError(\n                    f\"Number of samples in gts ({gt_array.shape[1]}) and number of given samples ({samples.size}) do not match\"\n                )\n\n        self._gts = gt_array\n        self._samples = samples\n\n    @property\n    def samples(self):\n        return self._samples\n\n    @property\n    def gt_values(self):\n        return numpy.ma.getdata(self._gts)\n\n    @property\n    def gt_ma_array(self):\n        return self._gts\n\n    @property\n    def missing_mask(self):\n        return numpy.ma.getmaskarray(self._gts)\n\n    @property\n    def shape(self):\n        return self._gts.shape\n\n    @property\n    def num_vars(self):\n        return self._gts.shape[0]\n\n    @property\n    def num_samples(self):\n        return self._gts.shape[1]\n\n    @property\n    def ploidy(self):\n        return self._gts.shape[2]\n\n    def get_vars(self, index):\n        gts = self.gt_ma_array[index, :, :]\n        gts.flags.writeable = False\n        return self.__class__(gt_array=gts, samples=self.samples)\n\n    def filter_samples_with_idxs(self, index):\n        gts = self.gt_ma_array[:, index, :]\n        samples = self.samples[index]\n\n        samples.flags.writeable = False\n        gts.flags.writeable = False\n        return self.__class__(gt_array=gts, samples=samples)\n\n    def filter_samples(self, samples: Sequence[str] | Sequence[int]) -> Self:\n        if self.samples is None:\n            raise ValueError(\"Cannot get samples from Genotypes without samples\")\n\n        if not isinstance(samples, SequenceABC):\n            raise ValueError(\"samples must be a sequence\")\n        index = numpy.where(numpy.isin(self.samples, samples))[0]\n        return self.filter_samples_with_idxs(index)\n\n    def to_012(self) -> numpy.ndarray:\n        res = _count_alleles_per_var(VariantsChunk(self), calc_freqs=False)\n        allele_counts = res[\"counts\"][DEF_POP_NAME][\"allele_counts\"].values\n\n        major_alleles = numpy.argmax(allele_counts, axis=1)\n        gts012 = numpy.sum(self.gt_values != major_alleles[:, None, None], axis=2)\n\n        gts012[numpy.any(self.missing_mask, axis=2)] = MISSING_ALLELE\n\n        return gts012\n\n\nArrayType = tuple[numpy.ndarray, pandas.DataFrame, pandas.Series, Genotypes]\n\n\ndef _normalize_pandas_types(dframe):\n    new_dframe = {}\n    for col, values in dframe.items():\n        if pandas.api.types.is_string_dtype(values):\n            values = pandas.Series(\n                values, dtype=pandas.StringDtype(PANDAS_STRING_STORAGE)\n            )\n        elif pandas.api.types.is_float_dtype(values):\n            values = pandas.Series(values, dtype=PANDAS_FLOAT_DTYPE())\n        elif pandas.api.types.is_integer_dtype(values):\n            values = pandas.Series(values, dtype=PANDAS_INT_DTYPE())\n        else:\n            raise ValueError(f\"Unsupported dtype for column {col}\")\n        values.flags.writeable = False\n        new_dframe[col] = values\n    return pandas.DataFrame(new_dframe)\n\n\nclass VariantsChunk:\n    def __init__(\n        self,\n        gts: Genotypes,\n        vars_info: pandas.DataFrame | None = None,\n        alleles: pandas.DataFrame | None = None,\n    ):\n        if not isinstance(gts, Genotypes):\n            raise ValueError(\"gts must be a Genotypes object\")\n        if vars_info is not None:\n            if vars_info.shape[0] != gts.num_vars:\n                raise ValueError(\n                    \"variants_info must have the same number of rows as gts\"\n                )\n            vars_info = _normalize_pandas_types(vars_info)\n\n        if alleles is not None:\n            if alleles.shape[0] != gts.num_vars:\n                raise ValueError(\"alleles must have the same number of rows as gts\")\n\n        self._arrays = {\"gts\": gts, \"vars_info\": vars_info, \"alleles\": alleles}\n        self._gt_array = gts\n        self._vars_info = vars_info\n        self._alleles = alleles\n\n    @property\n    def num_vars(self):\n        return self._arrays[\"gts\"].num_vars\n\n    @property\n    def num_samples(self):\n        return self._arrays[\"gts\"].num_samples\n\n    @property\n    def samples(self):\n        return self._arrays[\"gts\"].samples\n\n    @property\n    def ploidy(self):\n        return self._arrays[\"gts\"].ploidy\n\n    @property\n    def gts(self):\n        return self._arrays[\"gts\"]\n\n    @property\n    def vars_info(self):\n        return self._arrays[\"vars_info\"]\n\n    @property\n    def alleles(self):\n        return self._arrays[\"alleles\"]\n\n    def get_vars(self, index):\n        if not isinstance(index, slice):\n            index = list(index)\n        gts = self.gts.get_vars(index)\n        if self.vars_info is not None:\n            vars_info = self.vars_info.iloc[index, ...]\n        else:\n            vars_info = None\n        if self.alleles is not None:\n            alleles = self.alleles.loc[index, ...]\n        else:\n            alleles = None\n        return VariantsChunk(gts=gts, vars_info=vars_info, alleles=alleles)\n\n\nclass ChunkIterFactory(Protocol):\n    def _get_metadata(self):\n        pass\n\n    def iter_vars_chunks(self) -> Iterator[VariantsChunk]:\n        # It might raise a RuntimeError if the chunks can be iterated only once\n        pass\n\n\nclass FromGtChunkIterFactory:\n    def __init__(\n        self,\n        gts: Genotypes,\n        vars_info=None,\n    ):\n        chunk = VariantsChunk(gts=gts, vars_info=vars_info)\n        self._chunks = [chunk]\n\n    def _get_metadata(self):\n        first_chunk = self._chunks[0]\n        return {\n            \"samples\": first_chunk.gts.samples,\n            \"num_samples\": first_chunk.num_samples,\n            \"ploidy\": first_chunk.gts.ploidy,\n        }\n\n    def iter_vars_chunks(self) -> Iterator[VariantsChunk]:\n        return iter(self._chunks)\n\n\nclass Variants:\n    def __init__(\n        self,\n        vars_chunk_iter_factory: ChunkIterFactory,\n        desired_num_vars_per_chunk=DEF_NUM_VARS_PER_CHUNK,\n    ):\n        self.desired_num_vars_per_chunk = desired_num_vars_per_chunk\n        self._vars_chunks_iter_factory = vars_chunk_iter_factory\n        self._samples = None\n        self._num_samples = None\n        self._ploidy = None\n\n    def _get_orig_vars_iter(self):\n        return self._vars_chunks_iter_factory.iter_vars_chunks()\n\n    def iter_vars_chunks(self) -> Iterator[VariantsChunk]:\n        return _resize_chunks(\n            self._get_orig_vars_iter(), desired_num_rows=self.desired_num_vars_per_chunk\n        )\n\n    def _get_metadata(self):\n        return self._vars_chunks_iter_factory._get_metadata()\n\n    @property\n    def samples(self):\n        return self._get_metadata()[\"samples\"]\n\n    @property\n    def num_samples(self):\n        return self._get_metadata()[\"num_samples\"]\n\n    @property\n    def ploidy(self):\n        return self._get_metadata()[\"ploidy\"]\n\n    @classmethod\n    def from_gt_array(\n        cls,\n        gts: numpy.ndarray | numpy.ma.masked_array,\n        samples: list[str] | None = None,\n        vars_info: pandas.DataFrame | None = None,\n    ) -> Self:\n        if not numpy.ma.isarray(gts):\n            missing_mask = gts == MISSING_ALLELE\n            gts = numpy.ma.array(gts, mask=missing_mask, fill_value=MISSING_ALLELE)\n            skip_mask_check = True\n        else:\n            skip_mask_check = False\n        return cls(\n            vars_chunk_iter_factory=FromGtChunkIterFactory(\n                Genotypes(gts, skip_mask_check=skip_mask_check, samples=samples),\n                vars_info=vars_info,\n            )\n        )\n\n\ndef _concat_genotypes(genotypes: Sequence[Genotypes]):\n    gtss = []\n    for gts in genotypes:\n        if gts.samples != genotypes[0].samples:\n            raise ValueError(\"All genotypes must have the same samples\")\n        gtss.append(gts.gt_array)\n    gts = numpy.vstack(gtss)\n    return Genotypes(genotypes=gts, samples=genotypes[0].samples)\n\n\ndef _concatenate_arrays(arrays: list[ArrayType]) -> ArrayType:\n    if isinstance(arrays[0], numpy.ndarray):\n        array = numpy.vstack(arrays)\n    elif isinstance(arrays[0], pandas.DataFrame):\n        array = pandas.concat(arrays, axis=0)\n    elif isinstance(arrays[0], pandas.Series):\n        array = pandas.concat(arrays)\n    elif isinstance(arrays[0], Genotypes):\n        array = _concat_genotypes(arrays)\n    else:\n        raise ValueError(\"unknown type for array: \" + str(type(arrays[0])))\n    return array\n\n\ndef _concatenate_chunks(chunks: list[VariantsChunk]):\n    chunks = list(chunks)\n\n    if len(chunks) == 1:\n        return chunks[0]\n\n    arrays_to_concatenate = {\"gts\": [], \"vars_info\": [], \"alleles\": []}\n    for chunk in chunks:\n        if chunk.gts:\n            arrays_to_concatenate[\"gts\"].append(chunk.gts)\n        if chunk.vars_info:\n            arrays_to_concatenate[\"vars_info\"].append(chunk.vars_info)\n        if chunk.alleles:\n            arrays_to_concatenate[\"alleles\"].append(chunk.alleles)\n\n    num_arrays = [len(arrays) for arrays in arrays_to_concatenate.values()]\n    if not all([num_arrays[0] == len_ for len_ in num_arrays]):\n        raise ValueError(\"Not all chunks have the same arrays\")\n\n    concatenated_chunk = {}\n    for array_id, arrays in arrays_to_concatenate.items():\n        concatenated_chunk[array_id] = _concatenate_arrays(arrays)\n    concatenated_chunk = VariantsChunk(**concatenated_chunk)\n    return concatenated_chunk\n\n\ndef _get_num_rows_in_chunk(buffered_chunk):\n    if not buffered_chunk:\n        return 0\n    else:\n        return buffered_chunk.num_vars\n\n\ndef _fill_buffer(buffered_chunk, chunks, desired_num_rows):\n    num_rows_in_buffer = _get_num_rows_in_chunk(buffered_chunk)\n    if num_rows_in_buffer >= desired_num_rows:\n        return buffered_chunk, False\n\n    chunks_to_concat = []\n    if num_rows_in_buffer:\n        chunks_to_concat.append(buffered_chunk)\n\n    total_num_rows = num_rows_in_buffer\n    no_chunks_remaining = True\n    for chunk in chunks:\n        total_num_rows += chunk.num_vars\n        chunks_to_concat.append(chunk)\n        if total_num_rows >= desired_num_rows:\n            no_chunks_remaining = False\n            break\n\n    if not chunks_to_concat:\n        buffered_chunk = None\n    elif len(chunks_to_concat) > 1:\n        buffered_chunk = _concatenate_chunks(chunks_to_concat)\n    else:\n        buffered_chunk = chunks_to_concat[0]\n    return buffered_chunk, no_chunks_remaining\n\n\ndef _yield_chunks_from_buffer(buffered_chunk, desired_num_rows):\n    num_rows_in_buffer = _get_num_rows_in_chunk(buffered_chunk)\n    if num_rows_in_buffer == desired_num_rows:\n        chunks_to_yield = [buffered_chunk]\n        buffered_chunk = None\n        return buffered_chunk, chunks_to_yield\n\n    start_row = 0\n    chunks_to_yield = []\n    end_row = None\n    while True:\n        previous_end_row = end_row\n        end_row = start_row + desired_num_rows\n        if end_row <= num_rows_in_buffer:\n            chunks_to_yield.append(buffered_chunk.get_vars(slice(start_row, end_row)))\n        else:\n            end_row = previous_end_row\n            break\n        start_row = end_row\n\n    remainder = buffered_chunk.get_vars(slice(end_row, None))\n    buffered_chunk = remainder\n    return buffered_chunk, chunks_to_yield\n\n\ndef _resize_chunks(\n    chunks: Iterator[VariantsChunk], desired_num_rows\n) -> Iterator[VariantsChunk]:\n    buffered_chunk = None\n\n    while True:\n        # fill buffer with equal or more than desired\n        buffered_chunk, no_chunks_remaining = _fill_buffer(\n            buffered_chunk, chunks, desired_num_rows\n        )\n        # yield chunks until buffer less than desired\n        num_rows_in_buffer = _get_num_rows_in_chunk(buffered_chunk)\n        if not num_rows_in_buffer:\n            break\n        buffered_chunk, chunks_to_yield = _yield_chunks_from_buffer(\n            buffered_chunk, desired_num_rows\n        )\n        for chunk in chunks_to_yield:\n            yield chunk\n\n        if no_chunks_remaining:\n            yield buffered_chunk\n            break\n", "type": "text"}]